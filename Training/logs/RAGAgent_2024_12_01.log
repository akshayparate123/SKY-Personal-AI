2024-12-01 15:12:36 - __main__ - INFO - Model Name: BART
2024-12-01 15:12:36 - __main__ - INFO - Training Loss : []
2024-12-01 15:12:36 - __main__ - INFO - Validation Loss : []
2024-12-01 15:26:30 - __main__ - INFO - Batch Number : 1000, Training Loss : 6.987812872652288
2024-12-01 15:40:13 - __main__ - INFO - Batch Number : 2000, Training Loss : 6.907921144868182
2024-12-01 15:53:57 - __main__ - INFO - Batch Number : 3000, Training Loss : 6.810809858915449
2024-12-01 16:07:41 - __main__ - INFO - Batch Number : 4000, Training Loss : 6.721493704829684
2024-12-01 16:21:26 - __main__ - INFO - Batch Number : 5000, Training Loss : 6.640806459207769
2024-12-01 16:35:11 - __main__ - INFO - Batch Number : 6000, Training Loss : 6.562153895146409
2024-12-01 16:48:55 - __main__ - INFO - Batch Number : 7000, Training Loss : 6.476503147446859
2024-12-01 17:02:40 - __main__ - INFO - Batch Number : 8000, Training Loss : 6.390958133376758
2024-12-01 17:16:24 - __main__ - INFO - Batch Number : 9000, Training Loss : 6.308118216488947
2024-12-01 17:30:08 - __main__ - INFO - Saving checkpoint
2024-12-01 17:43:57 - __main__ - INFO - Batch Number : 11000, Training Loss : 6.150274792461761
2024-12-01 17:57:48 - __main__ - INFO - Batch Number : 12000, Training Loss : 6.073163784332012
2024-12-01 18:11:39 - __main__ - INFO - Batch Number : 13000, Training Loss : 5.99825580439543
2024-12-01 18:25:24 - __main__ - INFO - Batch Number : 14000, Training Loss : 5.925735312124276
2024-12-01 18:39:13 - __main__ - INFO - Batch Number : 15000, Training Loss : 5.8566732712327925
2024-12-01 18:53:26 - __main__ - INFO - Batch Number : 16000, Training Loss : 5.789566386238455
2024-12-01 19:07:32 - __main__ - INFO - Batch Number : 17000, Training Loss : 5.724499769452305
2024-12-01 19:21:33 - __main__ - INFO - Batch Number : 18000, Training Loss : 5.659102172396208
2024-12-01 19:35:34 - __main__ - INFO - Batch Number : 19000, Training Loss : 5.601212361361954
2024-12-01 19:49:33 - __main__ - INFO - Saving checkpoint
2024-12-01 20:03:49 - __main__ - INFO - Batch Number : 21000, Training Loss : 5.487395843450957
2024-12-01 20:18:00 - __main__ - INFO - Batch Number : 22000, Training Loss : 5.4342231086902
2024-12-01 20:32:10 - __main__ - INFO - Batch Number : 23000, Training Loss : 5.382169644654634
2024-12-01 20:46:23 - __main__ - INFO - Batch Number : 24000, Training Loss : 5.331541264381494
2024-12-01 21:00:56 - __main__ - INFO - Batch Number : 25000, Training Loss : 5.2817400732801785
2024-12-01 21:15:11 - __main__ - INFO - Batch Number : 26000, Training Loss : 5.233667760270178
2024-12-01 21:28:54 - __main__ - INFO - Batch Number : 27000, Training Loss : 5.188211836672187
2024-12-01 21:42:37 - __main__ - INFO - Batch Number : 28000, Training Loss : 5.144437333825767
2024-12-01 21:58:09 - __main__ - INFO - Batch Number : 29000, Training Loss : 5.101375772037949
2024-12-01 22:12:26 - __main__ - INFO - Saving checkpoint
2024-12-01 22:26:47 - __main__ - INFO - Batch Number : 31000, Training Loss : 5.020498430950788
2024-12-01 23:48:43 - __main__ - INFO - Model Name: BART
2024-12-01 23:48:43 - __main__ - INFO - Training Loss : []
2024-12-01 23:48:43 - __main__ - INFO - Validation Loss : []
2024-12-02 00:03:01 - __main__ - INFO - Batch Number : 1000, Training Loss : 0.7717489825187624
2024-12-02 00:17:22 - __main__ - INFO - Batch Number : 2000, Training Loss : 0.6412068620279454
2024-12-02 00:31:11 - __main__ - INFO - Batch Number : 3000, Training Loss : 0.5840634042245076
2024-12-02 00:44:55 - __main__ - INFO - Batch Number : 4000, Training Loss : 0.5468710587639058
2024-12-02 00:58:40 - __main__ - INFO - Batch Number : 5000, Training Loss : 0.5166535559379442
2024-12-02 01:12:22 - __main__ - INFO - Batch Number : 6000, Training Loss : 0.4932488389207689
2024-12-02 01:26:04 - __main__ - INFO - Batch Number : 7000, Training Loss : 0.4728816998831171
2024-12-02 01:39:47 - __main__ - INFO - Batch Number : 8000, Training Loss : 0.457485073217078
2024-12-02 01:53:30 - __main__ - INFO - Batch Number : 9000, Training Loss : 0.4439968489778464
2024-12-02 02:07:12 - __main__ - INFO - Saving checkpoint
2024-12-02 02:21:01 - __main__ - INFO - Batch Number : 11000, Training Loss : 0.42042698166501186
2024-12-02 02:34:43 - __main__ - INFO - Batch Number : 12000, Training Loss : 0.4106996395469889
2024-12-02 02:48:25 - __main__ - INFO - Batch Number : 13000, Training Loss : 0.40178656270521923
2024-12-02 03:02:07 - __main__ - INFO - Batch Number : 14000, Training Loss : 0.39322404533759814
2024-12-02 03:15:50 - __main__ - INFO - Batch Number : 15000, Training Loss : 0.384407379316748
2024-12-02 03:29:32 - __main__ - INFO - Batch Number : 16000, Training Loss : 0.37758912399209116
2024-12-02 03:43:13 - __main__ - INFO - Batch Number : 17000, Training Loss : 0.3713830519877569
2024-12-02 03:56:55 - __main__ - INFO - Batch Number : 18000, Training Loss : 0.36496782669818545
2024-12-02 04:10:38 - __main__ - INFO - Batch Number : 19000, Training Loss : 0.35864131916489833
2024-12-02 04:24:19 - __main__ - INFO - Saving checkpoint
2024-12-02 04:38:05 - __main__ - INFO - Batch Number : 21000, Training Loss : 0.3475961703281165
2024-12-02 04:51:47 - __main__ - INFO - Batch Number : 22000, Training Loss : 0.34216176736608
2024-12-02 05:05:29 - __main__ - INFO - Batch Number : 23000, Training Loss : 0.33730458580551886
2024-12-02 05:19:11 - __main__ - INFO - Batch Number : 24000, Training Loss : 0.3329458475345115
2024-12-02 05:32:53 - __main__ - INFO - Batch Number : 25000, Training Loss : 0.3285445439732853
2024-12-02 05:46:35 - __main__ - INFO - Batch Number : 26000, Training Loss : 0.3245797998239538
2024-12-02 06:00:17 - __main__ - INFO - Batch Number : 27000, Training Loss : 0.32073450772928735
2024-12-02 06:13:59 - __main__ - INFO - Batch Number : 28000, Training Loss : 0.3169499582963484
2024-12-02 06:27:41 - __main__ - INFO - Batch Number : 29000, Training Loss : 0.31356220589215605
2024-12-02 06:41:23 - __main__ - INFO - Saving checkpoint
2024-12-02 06:55:09 - __main__ - INFO - Batch Number : 31000, Training Loss : 0.30649075913463053
2024-12-02 07:08:51 - __main__ - INFO - Batch Number : 32000, Training Loss : 0.3032338751143495
2024-12-02 07:22:33 - __main__ - INFO - Batch Number : 33000, Training Loss : 0.3002578311672799
2024-12-02 07:36:14 - __main__ - INFO - Batch Number : 34000, Training Loss : 0.2968080842117034
2024-12-02 07:49:55 - __main__ - INFO - Batch Number : 35000, Training Loss : 0.29383782375617645
2024-12-02 08:03:37 - __main__ - INFO - Batch Number : 36000, Training Loss : 0.2912930391832045
2024-12-02 08:17:18 - __main__ - INFO - Batch Number : 37000, Training Loss : 0.2883314675265405
2024-12-02 08:31:00 - __main__ - INFO - Batch Number : 38000, Training Loss : 0.28555008877345306
2024-12-02 08:44:41 - __main__ - INFO - Batch Number : 39000, Training Loss : 0.28310194455559234
2024-12-02 08:58:27 - __main__ - INFO - Saving checkpoint
2024-12-02 09:12:42 - __main__ - INFO - Batch Number : 41000, Training Loss : 0.27825599043505794
2024-12-02 09:26:50 - __main__ - INFO - Batch Number : 42000, Training Loss : 0.2755502667634606
2024-12-02 09:40:58 - __main__ - INFO - Batch Number : 43000, Training Loss : 0.2732375259036101
2024-12-02 09:54:56 - __main__ - INFO - Batch Number : 44000, Training Loss : 0.2707740844805352
2024-12-02 10:08:39 - __main__ - INFO - Batch Number : 45000, Training Loss : 0.26859014548815113
2024-12-02 10:22:39 - __main__ - INFO - Batch Number : 46000, Training Loss : 0.2663766602311156
2024-12-02 10:36:36 - __main__ - INFO - Batch Number : 47000, Training Loss : 0.2642626268820678
2024-12-02 10:50:21 - __main__ - INFO - Batch Number : 48000, Training Loss : 0.2620220761805927
2024-12-02 11:04:05 - __main__ - INFO - Batch Number : 49000, Training Loss : 0.25998599963237096
2024-12-02 11:17:48 - __main__ - INFO - Saving checkpoint
2024-12-02 11:32:07 - __main__ - INFO - Batch Number : 51000, Training Loss : 0.255893228780949
2024-12-02 11:46:20 - __main__ - INFO - Batch Number : 52000, Training Loss : 0.2540239441316877
2024-12-02 12:00:33 - __main__ - INFO - Batch Number : 53000, Training Loss : 0.25256197489001014
2024-12-02 12:14:41 - __main__ - INFO - Batch Number : 54000, Training Loss : 0.2509179716421956
2024-12-02 12:28:24 - __main__ - INFO - Batch Number : 55000, Training Loss : 0.2490368078426154
2024-12-02 12:42:07 - __main__ - INFO - Batch Number : 56000, Training Loss : 0.24729610895892504
2024-12-02 12:55:53 - __main__ - INFO - Batch Number : 57000, Training Loss : 0.24543817059576487
2024-12-02 13:10:23 - __main__ - INFO - Batch Number : 58000, Training Loss : 0.24377207365604747
2024-12-02 13:24:17 - __main__ - INFO - Batch Number : 59000, Training Loss : 0.24209186242413575
2024-12-02 13:38:00 - __main__ - INFO - Saving checkpoint
2024-12-02 13:51:48 - __main__ - INFO - Batch Number : 61000, Training Loss : 0.23890514505849667
2024-12-02 14:05:31 - __main__ - INFO - Batch Number : 62000, Training Loss : 0.2373620140782656
2024-12-02 14:19:15 - __main__ - INFO - Batch Number : 63000, Training Loss : 0.2359250921569058
2024-12-02 14:32:59 - __main__ - INFO - Batch Number : 64000, Training Loss : 0.23442493708243223
2024-12-02 14:46:42 - __main__ - INFO - Batch Number : 65000, Training Loss : 0.23284642348006568
2024-12-02 15:00:25 - __main__ - INFO - Batch Number : 66000, Training Loss : 0.2313710261101073
2024-12-02 15:14:09 - __main__ - INFO - Batch Number : 67000, Training Loss : 0.2300592991936348
2024-12-02 15:28:35 - __main__ - INFO - Batch Number : 68000, Training Loss : 0.22859824307947027
2024-12-02 15:43:21 - __main__ - INFO - Batch Number : 69000, Training Loss : 0.22708619661217588
2024-12-02 15:57:05 - __main__ - INFO - Saving checkpoint
2024-12-02 16:10:52 - __main__ - INFO - Batch Number : 71000, Training Loss : 0.22444652568410034
2024-12-02 16:24:34 - __main__ - INFO - Batch Number : 72000, Training Loss : 0.22315132853144928
2024-12-02 16:37:37 - __main__ - INFO - Train loss 0.222032402088139
2024-12-02 16:42:22 - __main__ - INFO - Batch Number : 1000, Validation Loss : 0.7860958287557522
2024-12-02 16:47:06 - __main__ - INFO - Batch Number : 2000, Validation Loss : 0.7766408040367443
2024-12-02 16:51:50 - __main__ - INFO - Batch Number : 3000, Validation Loss : 0.771441890190578
2024-12-02 16:56:34 - __main__ - INFO - Batch Number : 4000, Validation Loss : 0.769344158681057
2024-12-02 17:01:18 - __main__ - INFO - Batch Number : 5000, Validation Loss : 0.76960112062711
2024-12-02 17:06:04 - __main__ - INFO - Batch Number : 6000, Validation Loss : 0.7702042284711681
2024-12-02 17:10:49 - __main__ - INFO - Batch Number : 7000, Validation Loss : 0.7719058645289211
2024-12-02 17:15:34 - __main__ - INFO - Batch Number : 8000, Validation Loss : 0.7745770105826468
2024-12-02 17:19:21 - __main__ - INFO - Validation loss 0.7739323006732143
2024-12-02 17:19:22 - __main__ - INFO - Model Saved
2024-12-02 17:19:22 - __main__ - INFO - Calculating Rouge Score of the model...
2024-12-02 17:20:22 - __main__ - INFO - Model Testing Complete
1)rogue_score_1:0.03555094894641139
2)rogue_score_L:0.033752143862117265
2024-12-02 17:20:22 - __main__ - INFO - Training Loss : [0.222032402088139]
2024-12-02 17:20:22 - __main__ - INFO - Validation Loss : [0.7739323006732143]

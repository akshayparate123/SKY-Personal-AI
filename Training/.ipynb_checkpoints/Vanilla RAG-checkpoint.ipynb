{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c725e54-05a1-4c9a-99a0-d22bbe85a8b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade \"protobuf>3.20\"\n",
    "# !pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a379d2-7c82-44cd-afe7-ae1e749ec892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from threading import Thread\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "chroma_client = chromadb.PersistentClient(path=\"./\")\n",
    "from googlesearch import search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362406fe-3022-4d03-987e-bf2e1a6ac1ac",
   "metadata": {},
   "source": [
    "## Vanilla RAG Pipeline using Chroma Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f969b7a-e88b-4405-8d2c-248b23963f2e",
   "metadata": {},
   "source": [
    "**User Query:** The process starts with a query from the user.\n",
    "\n",
    "**Retrieval Step:** Relevant documents or knowledge snippets are retrieved from a database/internet or document store using the query. Common retrieval methods include embeddings and similarity search.\n",
    "\n",
    "**Candidate Selection:** The top retrieved documents are selected based on relevance to the query.\n",
    "\n",
    "**Augmentation:** The selected documents are combined with the query to form an augmented input.\n",
    "\n",
    "**Generation with LLM:** The augmented input is passed to a Large Language Model (LLM), which generates a response using both the query and the retrieved documents.\n",
    "\n",
    "**Response Delivery:** The generated answer is provided back to the user, enriched with relevant information from the retrieval step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e488c1af-51a5-4674-a23d-b8dc88db0893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not available\n"
     ]
    }
   ],
   "source": [
    "model_exists = False\n",
    "if os.path.exists('../Saved_Models/Sky/fine-tuned-bert-sentiment_2024_10_04_0'):   \n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_name = '../Saved_Models/Sky/fine-tuned-bert-sentiment_2024_10_04_0'\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "    model_exists = True\n",
    "else:\n",
    "    print(\"Model not available\")\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddfc833-c36c-4103-a16a-cc8a5660e49f",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf38a1b-1a84-49e7-a7c2-b93b6f7a0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This piece of code will clear any text which is in latex format.\n",
    "\n",
    "def clean_latex(text):\n",
    "    # Remove LaTeX commands like {\\\\displaystyle}, {\\\\text{}}, etc.\n",
    "    cleaned_text = re.sub(r'\\\\displaystyle|\\\\text\\{.*?\\}', '', text)\n",
    "    \n",
    "    # Remove any LaTeX curly braces and unnecessary whitespaces\n",
    "    cleaned_text = re.sub(r'\\\\[a-z]+|{|}', '', cleaned_text)\n",
    "    \n",
    "    # Replace LaTeX-specific representations like \\\\dots with their equivalent\n",
    "    cleaned_text = re.sub(r'\\\\dots', '...', cleaned_text)\n",
    "    \n",
    "    # Remove multiple spaces introduced by LaTeX removal\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c86d4f3-edb3-47d4-b03d-b68dd637592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class contains the code to generate multiple threads.\n",
    "\n",
    "class ThreadWithReturnValue(Thread):\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs={}, Verbose=None):\n",
    "        Thread.__init__(self, group, target, name, args, kwargs)\n",
    "        self._return = None\n",
    "\n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args,\n",
    "                                                **self._kwargs)\n",
    "    def join(self, *args):\n",
    "        Thread.join(self, *args)\n",
    "        return self._return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21937f70-d5e8-4699-8aee-5a4acd65aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(cleaned_data):\n",
    "    ids_list = []\n",
    "    final_chunks = []\n",
    "    random_number = random.randint(0,10000000000)\n",
    "    loop = math.ceil(len(cleaned_data)/2000)\n",
    "    for i in range(0,loop):\n",
    "        if i ==0:\n",
    "            final_chunks.append(cleaned_data[(i*2000):(i+1)*2000])\n",
    "        else:\n",
    "            final_chunks.append(cleaned_data[(i*2000)-500:(i+1)*2000])\n",
    "        ids_list.append(str(random_number+(i/50)))\n",
    "    return (final_chunks,ids_list)\n",
    "    # return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082e8261-7f77-43db-bb06-c37bdf97251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Mining function is used to scare the data from websites.\n",
    "def data_mining(website):\n",
    "    filtered_content = \"\"\n",
    "    if (\".gov\" not in website) and (\"linkedin.com\" not in website) and (\"reddit.com\" not in website): #Ignore gov, linkedin and reddit websites\n",
    "        URL = website\n",
    "        r = requests.get(URL) \n",
    "        soup = BeautifulSoup(r.content, 'html5lib')\n",
    "        for tag in soup(['nav', 'header', 'footer', 'script', 'style', 'aside']): #Remove the information which is not useful\n",
    "            tag.decompose()\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li','strong']):  # Find all header and paragraph tags\n",
    "            filtered_content = filtered_content+tag.get_text()\n",
    "        remove_latex = clean_latex(filtered_content)                             #Remove all latex text by calling clean latex function which is defined above\n",
    "        chunks = create_chunks(remove_latex)                                     #Divide the complete paragraph into chunks\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d94dbc-f4d6-43ed-804e-60cf322d2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_search_links(query):             #Get links from google\n",
    "    return [link for link in search(query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6bbd08d-f753-4d51-bcf6-66565a9ef153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning using PySpark (Uncomment only when the data is in millions)\n",
    "\n",
    "# spark.stop()\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config(\"spark.driver.memory\", \"1g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"1g\") \\\n",
    "#     .appName(\"WebScrapingWithPySpark\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# cleaned_data_udf = udf(clean_latex, StringType())\n",
    "# # create_chunks_udf = udf(create_chunks, ArrayType(StringType()), ArrayType(StringType()))\n",
    "# schema = StructType([\n",
    "#     StructField(\"chunks\", ArrayType(StringType())),\n",
    "#     StructField(\"ids\", ArrayType(StringType()))\n",
    "# ])\n",
    "# create_chunks_udf = udf(create_chunks, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7669b2b9-20c6-4847-a9f0-a719bdb5df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")   #Create collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9963382-6a6f-4718-977f-b1bd3a81dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Examples\n",
    "query = [\"How to make crossiant?\",\"What is Big Data?\",\"Which of the following is a Characteristic of BigDdata?\",\"What is Hadoop?\",\"What is the Function of RAM?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "357fc609-eb9c-4b2e-a60b-3a8a8b14bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = query[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2576fb34-3e2f-4892-b6f2-c5ea548e65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################Data Fecthing################################\n",
    "def fetch_data(q):\n",
    "    website_links = get_google_search_links(q)                                                                 #Get Link\n",
    "    threads_list = [ThreadWithReturnValue(target=data_mining, args=(website,)) for website in website_links[:5]] #Create 5 threads\n",
    "    [thread.start() for thread in threads_list]                                                                  #Start the threads\n",
    "    fetched_data = [thread.join() for thread in threads_list]                                                    #Call the function to fetch the data\n",
    "    return fetched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d095e67-479a-425e-9c04-d7afef084a63",
   "metadata": {},
   "source": [
    "#### RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a62f51a7-f066-4e27-b27b-5a43c20e76eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot fetch the data currently since google doesnt allow data fetching through public wifi\n",
      "Distances :  [[0.4907780587673187, 0.49605846405029297]]\n",
      "LLM Model is unavailable\n",
      "Retrieved Chunk from the chroma vector database : ['What Is Big Data? Big Data ExplainedBig data has become more than a buzzword as information has grown more complex and vast in quantity and organizations struggling to gather, curate, understand, and use data effectively. It also describes challenges in IT, business, as well as emerging analytics technologies. But where did the term come from, how can you use big data at your organization, and how can you advance your big data analytics strategies? We’ll address these questions and provide tips to get started using your big data.History of big dataIn the 1960s, the United States created a large data center to store millions of tax records. This data center was the first real use case of digital data management. Through the 1990s and 2000s, leaders in the data space worried that existing technology would not be able to store large amounts of information produced by businesses, by government, and by people around the world. An even larger worry: would anyone be able to make sense of that much data. Today, thanks to technology innovations and more sophisticated analytics capabilities, it’s become cheaper and easier to store data and then analyze it. Now, concerns have shifted to effectively using big data in many formats such as structured, unstructured, and semi-structured, to inform business decisions.The four Vs of big dataData professionals describe big data by the four “Vs.” These characteristics are what make big data a big deal. The four Vs distinguish and define big data and describe its challenges.1. VolumeThe most well-known characteristic of big data is the volume generated. Businesses have grappled with the ever-increasing amounts of data for years. However, now it’s possible to store data for pennies on the dollar using data lakes or data warehouses like Snowflake. Businesses prioritize data organization with platforms like Hadoop, but it’s important to develop policies that standardize how long users keep data, and then a procedure for deleting or archivi', 'What is Big Data?Big Data refers to a voluminous amount of structured, semi-structured, or unstructured data that holds potential value but is challenging to process, analyze, and interpret using traditional data-processing methods due to its sheer size and complexity.Big Data transcends the capabilities of commonly used software and data management tools, requiring specialized systems and software to reveal trends, patterns, and associations - primarily relating to human behavior and interactions.What Constitutes Big Data?Big Data is characterized not just by the sheer volume, but also by its variety and the velocity at which it can be processed. It can be generated from various sources such as social networks, business transactions, machine-to-machine data, or data generated from sensory technologies. Big Data is diverse, encompassing various data types including text, images, audio, video; plus it often comes from globally distributed sources.The Three Vs of Big DataThe concept of Big Data is often described using three key characteristics, known as the \"Three Vs\":Volume: The amount of data. Given the exponential growth of data creation through various channels, the volume of data has reached unprecedented levels, often counting in the range of petabytes and exabytes.Velocity: The speed at which data is generated and processed. In the era of real-time information, data streams in at an extraordinary speed and must be dealt with promptly to extract timely insights.Variety: The range and type of data sources. Big Data can come in all types of formats - from structured, numeric data in traditional databases to unstructured text documents, emails, videos, audios, stock ticker data, and financial transactions.Big Data in Real-World ScenariosBig Data permeates various sectors of our everyday life and work, aiding in making better decisions, improving operations, and providing personalized experiences. For example:Healthcare: In healthcare, Big Data is utilized for pred']\n",
      "Execution time: 0.8170878887176514 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "try:\n",
    "    fetched_data = fetch_data(q)\n",
    "    # print(\"Query : {}\".format(q))\n",
    "    \n",
    "    final_chunks = []\n",
    "    final_ids = []\n",
    "    for f in fetched_data:\n",
    "        for i in f[0]:\n",
    "            final_chunks.append(i)\n",
    "        for i in f[1]:\n",
    "            final_ids.append(i)\n",
    "\n",
    "    # # print(flat_list)\n",
    "    \n",
    "    collection.add(\n",
    "        documents=final_chunks,\n",
    "        ids=final_ids\n",
    "    )\n",
    "except Exception as httperr:\n",
    "    print(\"Cannot fetch the data currently since google doesnt allow data fetching through public wifi\")\n",
    "    \n",
    "results = collection.query(\n",
    "    query_texts=q, # Chroma will embed this for you\n",
    "    n_results=2 # how many results to return\n",
    ")\n",
    "print(\"Distances : \",results['distances'])\n",
    "\n",
    "if model_exists:\n",
    "    input_ids = tokenizer.encode(\"<context>{}agent_1:{}\".format(results[\"documents\"][0][0],q), return_tensors='pt',max_length=512, truncation=True).to(device)\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_length=128, \n",
    "        num_beams=10, \n",
    "        early_stopping=True, \n",
    "        no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
    "        num_return_sequences=1,  # Number of sequences to return\n",
    "        temperature=0.7,  # Sampling temperature\n",
    "        top_k=50,  # Top-K sampling\n",
    "        top_p=0.9  # Top-p (nucleus) sampling\n",
    "    )\n",
    "    resp = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(resp)\n",
    "else:\n",
    "    print(\"LLM Model is unavailable\")\n",
    "    print(\"Retrieved Chunk from the chroma vector database :\",results[\"documents\"][0])\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64a247-8e1f-4748-ae14-67e080f23d53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

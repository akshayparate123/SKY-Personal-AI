{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a379d2-7c82-44cd-afe7-ae1e749ec892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time\n",
    "from threading import Thread\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, col, arrays_zip\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "import math\n",
    "import random\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "chroma_client = chromadb.PersistentClient(path=\"./\")\n",
    "from pyspark.sql import functions as F\n",
    "import sqlite3\n",
    "import re\n",
    "import torch\n",
    "import findspark\n",
    "from googlesearch import search\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5784b-0f54-4cd7-9583-95f3f9262a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config(\"spark.driver.memory\", \"2g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"2g\") \\\n",
    "#     .appName(\"test\") \\\n",
    "#     .getOrCreate()\n",
    "# df = spark.createDataFrame([(1, 'Alice'), (2, 'Bob')], ['id', 'name'])\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4183c-ba9a-4713-a473-9652a6783eab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Context Based Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "080dcec0-f051-495e-8156-cfce3328e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = load_dataset(\"9wimu9/eli5_mult_answers_en_no_answer_in_context\") # in deep\n",
    "dataset_2 = load_dataset(\"mlxen/squad_1_1_smallcase_context\") #one word\n",
    "dataset_3 = load_dataset(\"nbtpj/multi-context-long-answer-dataset\") # in short\n",
    "dataset_8 = load_dataset(\"LasRuinasCirculares/sft_correct\")\n",
    "dataset_9 = load_dataset(\"robbiegwaldd/rephrase_train\") # all\n",
    "ds = load_dataset(\"addy88/nq-question-answeronly\")\n",
    "ds = load_dataset(\"iarfmoose/question_generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec2d2eb5-c9b1-4acd-97cc-020be5c8abcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'contexts', 'gold_answer'],\n",
       "        num_rows: 71236\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'contexts', 'gold_answer'],\n",
       "        num_rows: 7916\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "480eac2d-d4c0-453e-9d24-86ab9aefd0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = dataset_1[\"train\"][\"question\"]\n",
    "raw_context = dataset_1[\"train\"][\"contexts\"]\n",
    "modified_context = []\n",
    "for c in raw_context:\n",
    "    final_string = \"\"\n",
    "    for i in c:\n",
    "        final_string = final_string+\" \"+ i\n",
    "    modified_context.append(final_string)\n",
    "context = modified_context\n",
    "agent_2 = dataset_1[\"train\"][\"gold_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa842db5-a791-471f-b83c-bdeae45158e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1.extend(dataset_2[\"train\"][\"question\"])\n",
    "context.extend(dataset_2[\"train\"][\"context\"])\n",
    "agent_2.extend([i[\"text\"][0] for i in dataset_2[\"train\"][\"answers\"]])\n",
    "\n",
    "agent_1.extend([i.split(\"Question:\")[1].replace(\"Answer:\",\"\") for i in dataset_8[\"train\"][\"instruction\"]])\n",
    "context.extend([i.replace(\"Evidence:\",\"\").replace(\"**\",\"\").split(\"Question:\")[0].replace(\"\\n\",\"\") for i in dataset_8[\"train\"][\"instruction\"]])\n",
    "agent_2.extend(dataset_8[\"train\"][\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74b680-72dd-44c0-89af-0922731fdfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e107141a-a661-45a2-95d9-efae13f15f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelNames =[\"pubmed_qa_X_squad_num_channel_1_test\",\"pubmed_qa_X_squad_num_channel_1_train\",\"pubmed_qa_X_squad_num_channel_2_test\",\"pubmed_qa_X_squad_num_channel_2_train\",\"pubmed_qa_X_squad_num_channel_3_test\",\"pubmed_qa_X_squad_num_channel_3_train\",\"pubmed_qa_X_squad_num_channel_4_test\",\"pubmed_qa_X_squad_num_channel_4_train\",\"pubmed_qa_num_channel_1_test\",\"pubmed_qa_num_channel_1_train\",\"pubmed_qa_num_channel_2_test\",\"pubmed_qa_num_channel_2_train\",\"pubmed_qa_num_channel_3_test\",\"pubmed_qa_num_channel_3_train\",\"pubmed_qa_num_channel_4_test\",\"pubmed_qa_num_channel_4_train\",\"squad_num_channel_1_test\",\"squad_num_channel_1_train\",\"squad_num_channel_2_test\",\"squad_num_channel_2_train\",\"squad_num_channel_3_test\",\"squad_num_channel_3_train\",\"squad_num_channel_4_test\",\"squad_num_channel_4_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdf94cdd-d86a-4a5a-82b6-2397553294e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = []\n",
    "cont = []\n",
    "answer = []\n",
    "for channel in channelNames:\n",
    "    question.extend([i[0].split(\"<||||>\")[0] for i in dataset_3[channel][\"context\"]])\n",
    "    cont.extend([i[0].split(\"<||||>\")[1] for i in dataset_3[channel][\"context\"]])\n",
    "    answer.extend([i for i in dataset_3[channel][\"answer\"]])\n",
    "temp_df = pd.DataFrame()\n",
    "temp_df[\"question\"] = question\n",
    "temp_df[\"cont\"] = cont\n",
    "temp_df[\"answer\"] = answer\n",
    "df = temp_df.drop_duplicates(subset=['question','cont','answer'], keep='first')\n",
    "agent_1.extend(df[\"question\"].tolist())\n",
    "context.extend(df[\"cont\"].tolist())\n",
    "agent_2.extend(df[\"answer\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44ec1e40-ac8b-4813-997e-b1af74f2a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"bart-base-squad.end2end.amazon\",\"bart-base-squad.end2end.new_wiki\",\"bart-base-squad.end2end.nyt\",\"bart-base-squad.end2end.reddit\",\"bart-base-squad.multitask.amazon\",\"bart-base-squad.multitask.new_wiki\",\"bart-base-squad.multitask.nyt\",\"bart-base-squad.multitask.reddit\",\"bart-base-squad.pipeline.amazon\",\"bart-base-squad.pipeline.new_wiki\",\"bart-base-squad.pipeline.nyt\",\"bart-base-squad.pipeline.reddit\",\"bart-base-squad.qg_reference.amazon\",\"bart-base-squad.qg_reference.new_wiki\",\"bart-base-squad.qg_reference.nyt\",\"bart-base-squad.qg_reference.reddit\",\"bart-large-squad.end2end.amazon\",\"bart-large-squad.end2end.new_wiki\",\"bart-large-squad.end2end.nyt\",\"bart-large-squad.end2end.reddit\",\"bart-large-squad.multitask.amazon\",\"bart-large-squad.multitask.new_wiki\",\"bart-large-squad.multitask.nyt\",\"bart-large-squad.multitask.reddit\",\"bart-large-squad.pipeline.amazon\",\"bart-large-squad.pipeline.new_wiki\",\"bart-large-squad.pipeline.nyt\",\"bart-large-squad.pipeline.reddit\",\"bart-large-squad.qg_reference.amazon\",\"bart-large-squad.qg_reference.new_wiki\",\"bart-large-squad.qg_reference.nyt\",\"bart-large-squad.qg_reference.reddit\",\"t5-base-squad.end2end.amazon\",\"t5-base-squad.end2end.new_wiki\",\"t5-base-squad.end2end.nyt\",\"t5-base-squad.end2end.reddit\",\"t5-base-squad.multitask.amazon\",\"t5-base-squad.multitask.new_wiki\",\"t5-base-squad.multitask.nyt\",\"t5-base-squad.multitask.reddit\",\"t5-base-squad.pipeline.amazon\",\"t5-base-squad.pipeline.new_wiki\",\"t5-base-squad.pipeline.nyt\",\"t5-base-squad.pipeline.reddit\",\"t5-base-squad.qg_reference.amazon\",\"t5-base-squad.qg_reference.new_wiki\",\"t5-base-squad.qg_reference.nyt\",\"t5-base-squad.qg_reference.reddit\",\"t5-large-squad.end2end.amazon\",\"t5-large-squad.end2end.new_wiki\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ccd3579-5197-4e97-9d2d-520ca4b7ddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:00<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0,len(datasets))):\n",
    "    # print(f'\\rDataset Name: {dataset}', end='', flush=True)\n",
    "    ds = load_dataset(\"lmqg/qa_squadshifts_synthetic\", datasets[i])\n",
    "    agent_1.extend(i for i in ds[\"train\"][\"question\"])\n",
    "    context.extend(i for i in ds[\"train\"][\"context\"])\n",
    "    agent_2.extend(i[\"text\"][0] for i in ds[\"train\"][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17a8e6cd-5941-4807-901d-2b63795f09f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'original_snippet', 'rephrased_snippet'],\n",
       "        num_rows: 291032\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33933026-2739-45fb-b51c-4954b0d4710b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1469571"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4e96e63-3dcf-48b9-9c30-d6c38f9c866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291032/291032 [00:00<00:00, 369566.84it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "conte = []\n",
    "split_qa = [i.split(\"\\n\\n\") for i in dataset_9[\"train\"][\"rephrased_snippet\"]]\n",
    "cont = [i.replace(\"\\n\",\"\") for i in dataset_9[\"train\"][\"original_snippet\"]]\n",
    "for i in tqdm(range(0,len(split_qa))):\n",
    "    q = [];a = [];c = []\n",
    "    if \"\\nAnswer\" not in split_qa[i][0]:\n",
    "        if len(split_qa[i]) % 2 != 0:\n",
    "            split_qa[i] = split_qa[i][:-1]\n",
    "        for j in range(0,len(split_qa[i]),2):\n",
    "            q.append(split_qa[i][j])\n",
    "            a.append(split_qa[i][j+1])\n",
    "            c.append(cont[i])\n",
    "    else:\n",
    "        new_split = [j.split(\"\\n\") for j in split_qa[i]]\n",
    "        for j in range(0,len(new_split)):\n",
    "            if len(new_split[j]) %2 !=0:\n",
    "                continue\n",
    "            q.append(new_split[j][0].replace(\"Question:\",\"\"))\n",
    "            a.append(new_split[j][1].replace(\"Answer:\",\"\"))\n",
    "            c.append(cont[i])\n",
    "    answers.extend(a)\n",
    "    questions.extend(q)\n",
    "    conte.extend(c)\n",
    "    # print(len(answers),len(questions))\n",
    "    # print(split_qa[i])\n",
    "agent_1.extend(questions)\n",
    "context.extend(conte)\n",
    "agent_2.extend(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01a34bf1-b866-4fc3-9fab-b788012ca849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802143"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99e8d9de-e436-4064-a053-9f2b1a1f992f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802143"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9171f1e-c1f3-4a18-9f1f-124f3ba916a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802143"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0ef1d5f-ada4-4cd9-9fbb-a017a0a8180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame()\n",
    "temp_df[\"agent_1\"] = agent_1\n",
    "temp_df[\"context\"] = context\n",
    "temp_df[\"agent_2\"] = agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e09e709-eca8-4c04-9688-abcb852f7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv(\"../Data/CleanedDatasets/ContextBasedQuestions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b564f60-1d29-40ec-b265-e360fa3ef5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"../Data/CleanedDatasets/ContextBasedQuestions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e198ec01-a6fe-4902-8fea-9f314a08a239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_2</th>\n",
       "      <th>agent_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agent_2:These foods contain certain sugars tha...</td>\n",
       "      <td>&lt;context&gt;  Actually it is because most of peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agent_2:I'd answer the question but the defaul...</td>\n",
       "      <td>&lt;context&gt;  The decay of false vacuum is a fasc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agent_2:Essentially the events in Crimea threa...</td>\n",
       "      <td>&lt;context&gt;  National Sovereignty is a very big ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agent_2:To have as little protection as is saf...</td>\n",
       "      <td>&lt;context&gt;  Most helmets are pretty similar in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agent_2:Your eyes are magnificent sensors, and...</td>\n",
       "      <td>&lt;context&gt;  The reason is that a camera is basi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             agent_2  \\\n",
       "0  agent_2:These foods contain certain sugars tha...   \n",
       "1  agent_2:I'd answer the question but the defaul...   \n",
       "2  agent_2:Essentially the events in Crimea threa...   \n",
       "3  agent_2:To have as little protection as is saf...   \n",
       "4  agent_2:Your eyes are magnificent sensors, and...   \n",
       "\n",
       "                                             agent_1  \n",
       "0  <context>  Actually it is because most of peop...  \n",
       "1  <context>  The decay of false vacuum is a fasc...  \n",
       "2  <context>  National Sovereignty is a very big ...  \n",
       "3  <context>  Most helmets are pretty similar in ...  \n",
       "4  <context>  The reason is that a camera is basi...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6532a20-97ae-4c1a-902b-4d83dea290d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df[\"agent_2\"] = \"agent_2:\"+temp_df[\"agent_2\"]\n",
    "temp_df[\"agent_1\"] = \"<context> \" + temp_df[\"context\"] + \"agent_1:\"+temp_df[\"old_agent_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2508138-20d6-4649-9de9-848fc98da729",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.dropna(inplace = True)\n",
    "temp_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7aed2-0ec7-4dcb-a111-5113af5ca6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06f63fe5-368f-41c4-888a-e6dc0b81a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1799426 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1401 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 1799426/1799426 [45:44<00:00, 655.72it/s] \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "agent_1_extended = []\n",
    "agent_2_extended = []\n",
    "tokenizedLengthList = []\n",
    "idx = 0\n",
    "for input_text in tqdm(temp_df[\"agent_1\"].to_list()):\n",
    "    try:\n",
    "        input_tokenized = tokenizer(input_text,return_tensors='pt')\n",
    "        tokenizedLength = len(input_tokenized[\"input_ids\"][0])\n",
    "        tokenizedLengthList.append(tokenizedLength)\n",
    "        if tokenizedLength < 510:\n",
    "            agent_1_extended.append(input_text)\n",
    "            agent_2_extended.append(temp_df[\"agent_2\"][idx])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    idx = idx+1\n",
    "    # if tokenizedLength > 510 and tokenizedLength < 1500:\n",
    "        \n",
    "    #         num_of_loops = math.ceil(tokenizedLength/510)\n",
    "    #         slice_length = math.ceil(len(input_text)/num_of_loops)\n",
    "    #         for i in range(0,num_of_loops):\n",
    "    #             if i == 0:\n",
    "    #                 agent_1_extended.append(\"agent_1:\"+(input_text[i*slice_length:(i*slice_length)+slice_length])+\"<question>\"+temp_df[\"old_agent_1\"][idx])\n",
    "    #             else:\n",
    "    #                 agent_1_extended.append(\"agent_1:\"+\"<context>\"+(input_text[(i*slice_length)-50:(i*slice_length)+slice_length])+\"<question>\"+temp_df[\"old_agent_1\"][idx])\n",
    "    \n",
    "    # else:\n",
    "    #     agent_1_extended.append(\"agent_1:\"+input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43fbe6ac-67eb-46cb-ac4c-a6ccea026b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400831"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_1_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca6c8887-3b4a-4876-920c-00f8aa9e7baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400831"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_2_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84788807-f494-4ced-bbd9-31cee65eb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame()\n",
    "temp_df[\"agent_2\"] =agent_2_extended\n",
    "temp_df[\"agent_1\"] =agent_1_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cd8236-afd2-4c82-b885-c400b6a58fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<context>  National Sovereignty is a very big deal so anytime a country (especially a country with nukes) starts to reject previously agreed upon borders it is a big deal.\\n\\nThe reason you should care is that Russia has been for quite sometime trying to maintain some degree of influence over several former Soviet republics.  As some of these states move away from Russia, it is going to cause conflict and potentially a full blown war. likely to evolve into nothing\\n\\ncrimea was granted to ukraine as a restitution gift and well over half the population is in favour of annexation\\n\\nthey already speak russian and maintain russian culture\\n\\nputin is a total dick but once this blows over they will pay lower taxes to an equally corrupt government and receive additional social benefits\\n\\nyeah, its a huge problem that a part of a country is essentially being taken over but it hardly sets precedent for speculations of a third world war\\n\\nagent_1:what is the big deal about russia invading ukraine and taking over crimea, and what do i, as a us citizen have to worry about it?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_1_extended[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24687bb3-c02b-40dc-be77-d953121909ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv(\"../Data/CleanedDatasets/ContextBasedQuestions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cfbb514-ada6-4fae-a7ac-67bc8db4f902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<context> 5 best Java seeds for Minecraft 1.19 updateThe 1.19 update for Minecraft introduced many new features. \\xa0Two major biomes, Deep Dark and Mangrove Swamp, have been added to the game and are quickly becoming quite popular.The two biomes are arguably the biggest addition to the game. \\xa0Deep Dark and Mangrove Swamp both provide reasons to visit them, but there's only one problem: they can be difficult to find.Minecraft seeds to try in Java Edition version 1.19Multiple Blue Rings\\xa0Note: The 1.18 update introduced seed universality, so seeds will work on both versions with minimal differences. \\xa0The main differences are when moving from Java to Bedrock, but not vice versa.The ancient city in 1.19 is a new but extremely rare structure. \\xa0It's a challenge to trace a regular Deep Dark with just the Skulk and the Warden. \\xa0Most of those found do not even contain any ancient cities.Seed: -457009213479927390This seed puts players into an amazing spawn. \\xa0Gamers come right next to a village, which is always good for getting loot and other items.Seed: -6709148406763899126Good loot can be a dealbreaker for many Minecraft gamers. \\xa0If a particular seed has good loot, such as diamonds in a blacksmith's chest, players are more willing to try it. \\xa0This seed does not necessarily have that, but there is something even more rare in it.Seed: -2110863992403414331The seeds are devoid of a mangrove swamp village because they do not exist. \\xa0However, this village originates from swamps, so it matters.Seed: 1450778142214593647Mangrove swamps are a big part of the Minecraft 1.19 update, and players want to find it. \\xa0Luckily, this Java seed gives gamers that opportunity. \\xa0It also has a very unique biome generation.Seed: 6705098208300174216agent_1: What are the two major biomes in Minecraft 1.19 and how do they differ from Deep Dark and Mangrove Swamp?\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df[\"agent_1\"][1400700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2e83014-4dd7-4388-a9f4-4e48c932c981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agent_2: In Minecraft 1.19, there are two new biomes added: \"Deep Dark\" and \"Mangrove Swamp\". The Deep Dark biome provides reasons to visit it but can be difficult to find due to its location near a swamp. Meanwhile, the Mangrove Swamp biome has only one problem - it can be difficult to find as well. Minecraft seeds like -457009213479927390, -6709148406763899126, and -2110863992403414331 can be used to trace a Deep Dark with just the Skulk and Warden. These seeds contain good loot but do not necessarily have Mangrove Swamp as they are located in swamps instead of in the swamp biome. The seeds -2110863992403414331 and 1450778142214593647 can be used to find Mangrove Swamp by using a village as an alternative spawn point. The seeds -6709148406763899126 and 6705'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df[\"agent_2\"][1400700]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362406fe-3022-4d03-987e-bf2e1a6ac1ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RAG Pipeline using Chroma Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e488c1af-51a5-4674-a23d-b8dc88db0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = '../Saved_Models/Sky/fine-tuned-bert-sentiment_2024_10_04_0'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddfc833-c36c-4103-a16a-cc8a5660e49f",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf38a1b-1a84-49e7-a7c2-b93b6f7a0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_latex(text):\n",
    "    # Remove LaTeX commands like {\\\\displaystyle}, {\\\\text{}}, etc.\n",
    "    cleaned_text = re.sub(r'\\\\displaystyle|\\\\text\\{.*?\\}', '', text)\n",
    "    \n",
    "    # Remove any LaTeX curly braces and unnecessary whitespaces\n",
    "    cleaned_text = re.sub(r'\\\\[a-z]+|{|}', '', cleaned_text)\n",
    "    \n",
    "    # Replace LaTeX-specific representations like \\\\dots with their equivalent\n",
    "    cleaned_text = re.sub(r'\\\\dots', '...', cleaned_text)\n",
    "    \n",
    "    # Remove multiple spaces introduced by LaTeX removal\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c86d4f3-edb3-47d4-b03d-b68dd637592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadWithReturnValue(Thread):\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs={}, Verbose=None):\n",
    "        Thread.__init__(self, group, target, name, args, kwargs)\n",
    "        self._return = None\n",
    "\n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args,\n",
    "                                                **self._kwargs)\n",
    "    def join(self, *args):\n",
    "        Thread.join(self, *args)\n",
    "        return self._return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082e8261-7f77-43db-bb06-c37bdf97251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_mining(website):\n",
    "    filtered_content = \"\"\n",
    "    if (\".gov\" not in website) and (\"linkedin.com\" not in website) and (\"reddit.com\" not in website):\n",
    "        URL = website\n",
    "        r = requests.get(URL) \n",
    "        soup = BeautifulSoup(r.content, 'html5lib')\n",
    "        for tag in soup(['nav', 'header', 'footer', 'script', 'style', 'aside']):\n",
    "            tag.decompose()\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li','strong']):\n",
    "            filtered_content = filtered_content+tag.get_text()\n",
    "        remove_latex = clean_latex(filtered_content)\n",
    "        chunks = create_chunks(remove_latex)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d94dbc-f4d6-43ed-804e-60cf322d2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_search_links(query):\n",
    "    return [link for link in search(query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21937f70-d5e8-4699-8aee-5a4acd65aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(cleaned_data):\n",
    "    ids_list = []\n",
    "    final_chunks = []\n",
    "    random_number = random.randint(0,10000000000)\n",
    "    loop = math.ceil(len(cleaned_data)/2000)\n",
    "    for i in range(0,loop):\n",
    "        if i ==0:\n",
    "            final_chunks.append(cleaned_data[(i*2000):(i+1)*2000])\n",
    "        else:\n",
    "            final_chunks.append(cleaned_data[(i*2000)-500:(i+1)*2000])\n",
    "        ids_list.append(str(random_number+(i/50)))\n",
    "    return (final_chunks,ids_list)\n",
    "    # return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6bbd08d-f753-4d51-bcf6-66565a9ef153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config(\"spark.driver.memory\", \"1g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"1g\") \\\n",
    "#     .appName(\"WebScrapingWithPySpark\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# cleaned_data_udf = udf(clean_latex, StringType())\n",
    "# # create_chunks_udf = udf(create_chunks, ArrayType(StringType()), ArrayType(StringType()))\n",
    "# schema = StructType([\n",
    "#     StructField(\"chunks\", ArrayType(StringType())),\n",
    "#     StructField(\"ids\", ArrayType(StringType()))\n",
    "# ])\n",
    "# create_chunks_udf = udf(create_chunks, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7669b2b9-20c6-4847-a9f0-a719bdb5df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9963382-6a6f-4718-977f-b1bd3a81dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"How to make crossiant?\",\"What is Big Data?\",\"Which of the following is a Characteristic of BigDdata?\",\"What is Hadoop?\",\"What is the Function of RAM?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "357fc609-eb9c-4b2e-a60b-3a8a8b14bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = query[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2576fb34-3e2f-4892-b6f2-c5ea548e65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################Data Fecthing################################\n",
    "def fetch_data(q):\n",
    "    website_links = get_google_search_links(q)\n",
    "    threads_list = [ThreadWithReturnValue(target=data_mining, args=(website,)) for website in website_links[:5]]\n",
    "    [thread.start() for thread in threads_list]\n",
    "    fetched_data = [thread.join() for thread in threads_list]\n",
    "    return fetched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d095e67-479a-425e-9c04-d7afef084a63",
   "metadata": {},
   "source": [
    "#### RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a62f51a7-f066-4e27-b27b-5a43c20e76eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4907780587673187, 0.49605846405029297]]\n",
      "In the 1960s, the United States established a massive data center to store millions of tax records. This marked the beginning of digital data management. However, with advancements in technology and sophisticated analytics capabilities, businesses are now able to save large amounts of data for pennies on the dollar using data lakes or data warehouses like Snowflake. As a result, big data has become more complex and vast than ever before, making it challenging for organizations to gather, manage, understand, and utilize data effectively. It also presents challenges in IT, business, as well as emerging analytics technologies. To address these issues, we provide guidance on how\n",
      "Execution time: 3.7856297492980957 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "fetched_data = fetch_data(q)\n",
    "# print(\"Query : {}\".format(q))\n",
    "\n",
    "final_chunks = []\n",
    "final_ids = []\n",
    "for f in fetched_data:\n",
    "    for i in f[0]:\n",
    "        final_chunks.append(i)\n",
    "    for i in f[1]:\n",
    "        final_ids.append(i)\n",
    "\n",
    "# # print(flat_list)\n",
    "\n",
    "collection.add(\n",
    "    documents=final_chunks,\n",
    "    ids=final_ids\n",
    ")\n",
    "results = collection.query(\n",
    "    query_texts=q, # Chroma will embed this for you\n",
    "    n_results=2 # how many results to return\n",
    ")\n",
    "print(results['distances'])\n",
    "\n",
    "\n",
    "input_ids = tokenizer.encode(\"<context>{}agent_1:{}\".format(results[\"documents\"][0][0],q), return_tensors='pt',max_length=512, truncation=True).to(device)\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=128, \n",
    "    num_beams=10, \n",
    "    early_stopping=True, \n",
    "    no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
    "    num_return_sequences=1,  # Number of sequences to return\n",
    "    temperature=0.7,  # Sampling temperature\n",
    "    top_k=50,  # Top-K sampling\n",
    "    top_p=0.9  # Top-p (nucleus) sampling\n",
    ")\n",
    "resp = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(resp)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d90b65c-94b8-495f-81fc-e0249440a9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2453"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e44fa-f33e-4ee4-b047-ef8db93aa491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = list(zip(website_links, fetched_data))\n",
    "# uncleaned_df = spark.createDataFrame(temp_df, [\"url\", \"uncleaned_data\"])\n",
    "# cleaned_df = uncleaned_df.withColumn(\"cleaned_data\", cleaned_data_udf(\"uncleaned_data\"))\n",
    "# clean_df = cleaned_df.withColumn(\"processed\", create_chunks_udf(\"cleaned_data\"))\n",
    "# clean_df = clean_df.select(\n",
    "#     F.col('processed.chunks').alias(\"chunks\"),\n",
    "#     F.col('processed.ids').alias(\"ids\")\n",
    "# )\n",
    "# chunks_df = clean_df.withColumn(\"zipped\", arrays_zip(\"chunks\", \"ids\")) \\\n",
    "#        .withColumn(\"exploded\", explode(\"zipped\")) \\\n",
    "#        .select(clean_df.chunks.alias(\"chunks_array\"), \"exploded.chunks\", \"exploded.ids\")\n",
    "# chunks_df = chunks_df.drop('chunks_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0443b1-3da4-4fa8-aff5-a3c15a17144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks_df.show()\n",
    "# pandas_df = chunks_df.toPandas()\n",
    "# collection.add(\n",
    "#     documents=list(pandas_df[\"chunks\"]),\n",
    "#     ids=list(pandas_df[\"ids\"])\n",
    "# )\n",
    "# results = collection.query(\n",
    "#     query_texts=[q], # Chroma will embed this for you\n",
    "#     n_results=2 # how many results to return\n",
    "# )\n",
    "# print(results['distances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1f6bc-eca9-4d5f-99e6-e5604ebbf010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Query : {}\".format(q))\n",
    "# input_ids = tokenizer.encode(\"<context>{}agent_1:{}\".format(results[\"documents\"][0][0],query), return_tensors='pt',max_length=1024, truncation=True).to(device)\n",
    "# output = model.generate(\n",
    "#     input_ids, \n",
    "#     max_length=512, \n",
    "#     num_beams=10, \n",
    "#     early_stopping=True, \n",
    "#     no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
    "#     num_return_sequences=1,  # Number of sequences to return\n",
    "#     temperature=0.7,  # Sampling temperature\n",
    "#     top_k=50,  # Top-K sampling\n",
    "#     top_p=0.9  # Top-p (nucleus) sampling\n",
    "# )\n",
    "# resp = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3913f4c0-37f2-4856-ae71-ec90ec86f56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : How to make crossiant?\n",
      "agent_2:A professional chef’s tips for making French croissants.\n",
      "\n",
      "Query : What is Big Data?\n",
      "The concept of big data encompasses a wide range of data sources and formats, including web logs, social media interactions, ecommerce and online transactions, financial transactions. However, due to increasing volume, velocity, and variety, traditional databases are unable to handle this challenge effectively. To overcome these challenges, organizations must employ cutting-edge technologies such as Hadoop, which allow them to collect and store large datasets while also analyzing them in order to uncover new and valuable insights. By doing so, companies can streamline their operations and increase productivity and competitiveness.\n",
      "\n",
      "Query : Which of the following is a Characteristic of BigDdata?\n",
      "agent_2:What are the characteristics of big data?\n",
      "1) There is a lot of data\n",
      "2) Big data formats change rapidly\n",
      "3) Which of the following is not an issue\n",
      "4) It can come from untrusted sources\n",
      "\n",
      "Query : What is Hadoop?\n",
      "The term \"Hadoop\" refers to a project and a software library. Any other usage is ill-defined. In addition to Apache hadoop definition from Official website, I would like to highlight that there are many sub-systems in the HadoOP ecosystem. Quoting this content from official website so that broken links in future does not cause any issue to this answer.\n",
      "\n",
      "Query : What is the Function of RAM?\n",
      "Many modern personal computers come equipped with RAM in the form of modules known as memory modules or DRAM modules, which are roughly the size of a few sticks of chewing gum. These can be easily replaced should they become damaged or when changing needs demand more storage capacity. When the system runs low on physical memory, it can \"swap\" portions of RAM to the paging file to make room for new data, as well as to read previously swapped information back into RAM. Virtual memory most modern operating systems employ a method of extending RAM capacity, commonly referred to as \"virtual memory.\" A portion of the computer's hard drive is set aside for a \"paging\" file or a scratch partition, and the combination of physical RAM and this file forms the total memory. This process, sometimes called shadowing, is fairly common in both computers and embedded systems.\n",
      "\n",
      "Execution time: 26.451351642608643 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 59166)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "# website_links = get_google_search_links(q)\n",
    "\n",
    "# ###############################SPARK################################\n",
    "\n",
    "# threads_list = [ThreadWithReturnValue(target=data_mining, args=(website,)) for website in website_links[:5]]\n",
    "# [thread.start() for thread in threads_list]\n",
    "# fetched_data = [thread.join() for thread in threads_list]\n",
    "\n",
    "# temp_df = list(zip(website_links, fetched_data))\n",
    "# uncleaned_df = spark.createDataFrame(temp_df, [\"url\", \"uncleaned_data\"])\n",
    "# cleaned_df = uncleaned_df.withColumn(\"cleaned_data\", cleaned_data_udf(\"uncleaned_data\"))\n",
    "# clean_df = cleaned_df.withColumn(\"processed\", create_chunks_udf(\"cleaned_data\"))\n",
    "\n",
    "# clean_df = clean_df.select(\n",
    "#     F.col('processed.chunks').alias(\"chunks\"),\n",
    "#     F.col('processed.ids').alias(\"ids\")\n",
    "# )\n",
    "\n",
    "# chunks_df = clean_df.withColumn(\"zipped\", arrays_zip(\"chunks\", \"ids\")) \\\n",
    "#        .withColumn(\"exploded\", explode(\"zipped\")) \\\n",
    "#        .select(clean_df.chunks.alias(\"chunks_array\"), \"exploded.chunks\", \"exploded.ids\")\n",
    "# chunks_df = chunks_df.drop('chunks_array')\n",
    "\n",
    "# # pandas_df = chunks_df.toPandas()\n",
    "# # conn = sqlite3.connect('chroma.sqlite3')  # 'example.db' is the SQLite database file\n",
    "# # pandas_df.to_sql('table_name', conn, if_exists='replace', index=False)\n",
    "# # conn.close()\n",
    "# # spark.stop()\n",
    "\n",
    "# ###########################CHROMA###################################\n",
    "\n",
    "# # chunks_df.show()\n",
    "# pandas_df = chunks_df.toPandas()\n",
    "# collection.add(\n",
    "#     documents=list(pandas_df[\"chunks\"]),\n",
    "#     ids=list(pandas_df[\"ids\"])\n",
    "# )\n",
    "# results = collection.query(\n",
    "#     query_texts=[q], # Chroma will embed this for you\n",
    "#     n_results=2 # how many results to return\n",
    "# )\n",
    "# # print(results['distances'])\n",
    "\n",
    "# ###########################LLM##################################\n",
    "\n",
    "# print(\"Query : {}\".format(q))\n",
    "# input_ids = tokenizer.encode(\"<context>{}agent_1:{}\".format(results[\"documents\"][0][0],query), return_tensors='pt',max_length=1024, truncation=True).to(device)\n",
    "# output = model.generate(\n",
    "#     input_ids, \n",
    "#     max_length=512, \n",
    "#     num_beams=10, \n",
    "#     early_stopping=True, \n",
    "#     no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
    "#     num_return_sequences=1,  # Number of sequences to return\n",
    "#     temperature=0.7,  # Sampling temperature\n",
    "#     top_k=50,  # Top-K sampling\n",
    "#     top_p=0.9  # Top-p (nucleus) sampling\n",
    "# )\n",
    "# resp = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(resp)\n",
    "# print()\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9d21733-2195-49ea-98a8-a62d1a63e413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "364eb77e-d642-494e-8572-bc8de7314424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output: agentagent>:Data and Data management Ben Lutkevich, Site Editor Ivy Wigmore, are the 3 V's of big data? The 3 V's (volume, velocity and variety) are three defining characteristics or dimensions of big data. Volume refers to the amount of data, velocity refers to the speed of data processing, and variety refers to the number of types of data. As to the 3 V's model, the challenges of big data management result from the expansion of all three properties, rather than just the volume alone or the sheer amount of data to be managed. An organization can be better equipped to deal with big data challenges through understanding the 3 V's of big data management. Gartner analyst Doug Laney introduced the 3 V's concept in a 2001 Meta Group research publication, \"3D Data Management: Controlling Data Volume, Velocity and Variety.\" More recently, additional V's have been proposed for addition to the model, including variability -- the increase in the range of values typical of a large data set -- and value, which addresses the need for valuation of enterprise data.Data using the 3 V's is sometimes referred to as 3D data. Why are the 3 V's important to big data? The 3 V's help define data and are how it's measured. Big data refers to data points that are generated frequently, in high volume and in multiple forms. These characteristics determine the data modeling techniques analysts use, such as how the data is processed and stored They also play part in determining the value data theory, a higher volume, and variety data more valuable it creates a stronger analytical basis stronger Data with little variety yieldvelocity Data with tooing provides understandingWhatWhat\n",
      "Confidence Score: 0.8939192891120911\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(\"<context>{}agent_1:{}\".format(results[\"documents\"][0][0],query), return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "\n",
    "# Forward pass through the model to get the logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get the predicted token ids from the logits (the most likely tokens)\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Calculate the probabilities (softmax) for the predicted tokens\n",
    "probabilities = softmax(logits, dim=-1)\n",
    "\n",
    "# Get the probabilities of the predicted tokens\n",
    "predicted_probabilities = probabilities.gather(2, predicted_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# Calculate the confidence score as the average probability of the predicted tokens\n",
    "confidence_score = predicted_probabilities.mean().item()\n",
    "\n",
    "print(\"Generated Output:\", tokenizer.decode(predicted_ids[0], skip_special_tokens=True).replace(\"\\n\",\"\"))\n",
    "print(\"Confidence Score:\", confidence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0105b-adfe-44ec-9801-724f68dfad6c",
   "metadata": {},
   "source": [
    "### PostgreSQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "652365a3-69b8-4f59-9ba7-465306223411",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"wikipedia\", \"20220301.simple\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd220540-dc93-460e-a06b-e631bf652c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"wikipedia\", \"20220301.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b1c152-a008-4723-97e0-2a7570cb7d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1002/1002 [00:00<00:00, 11189.84files/s]\n",
      "Generating train split:   0%|          | 0/12119256 [00:00<?, ? examples/s]Failed to read file 'C:\\Users\\Akshay\\.cache\\huggingface\\datasets\\downloads\\e58fcf18ad9da845d136da243447c169ff3e92b9e6c489432cb8b1a08329b0cb' with error <class 'datasets.table.CastError'>: Couldn't cast\n",
      "ORIGINAL_TEXT: string\n",
      "BATCH_RESPONSES_QA: string\n",
      "-- schema metadata --\n",
      "pandas: '{\"column_indexes\": [{\"field_name\": null, \"metadata\": null, \"name' + 543\n",
      "to\n",
      "{'text': Value(dtype='string', id=None)}\n",
      "because column names don't match\n",
      "Generating train split:   0%|          | 12120/12119256 [00:00<00:36, 327570.20 examples/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCastError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\datasets\\builder.py:1997\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1996\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 1997\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, table \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[0;32m   1998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m writer\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m>\u001b[39m max_shard_size:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\datasets\\packaged_modules\\parquet\\parquet.py:93\u001b[0m, in \u001b[0;36mParquet._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;66;03m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[39;00m\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;66;03m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[39;00m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;66;03m# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cast_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\datasets\\packaged_modules\\parquet\\parquet.py:71\u001b[0m, in \u001b[0;36mParquet._cast_table\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# more expensive cast to support nested features with keys in a different order\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# allows str <-> int/float or str to Audio for example\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mtable_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marrow_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa_table\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\datasets\\table.py:2302\u001b[0m, in \u001b[0;36mtable_cast\u001b[1;34m(table, schema)\u001b[0m\n\u001b[0;32m   2301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m!=\u001b[39m schema:\n\u001b[1;32m-> 2302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2303\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m!=\u001b[39m schema\u001b[38;5;241m.\u001b[39mmetadata:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\datasets\\table.py:2256\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[1;34m(table, schema)\u001b[0m\n\u001b[0;32m   2255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(table\u001b[38;5;241m.\u001b[39mcolumn_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28msorted\u001b[39m(features):\n\u001b[1;32m-> 2256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[0;32m   2257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(table\u001b[38;5;241m.\u001b[39mschema)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2258\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[0;32m   2259\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[0;32m   2260\u001b[0m     )\n\u001b[0;32m   2261\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()]\n",
      "\u001b[1;31mCastError\u001b[0m: Couldn't cast\nORIGINAL_TEXT: string\nBATCH_RESPONSES_QA: string\n-- schema metadata --\npandas: '{\"column_indexes\": [{\"field_name\": null, \"metadata\": null, \"name' + 543\nto\n{'text': Value(dtype='string', id=None)}\nbecause column names don't match",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChristophSchuhmann/wikipedia-en-chunks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\datasets\\load.py:2616\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[0;32m   2615\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 2616\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2622\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2624\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2625\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2626\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   2627\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\datasets\\builder.py:1029\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1028\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[1;32m-> 1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1030\u001b[0m         dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[0;32m   1031\u001b[0m         verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m   1032\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m   1033\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m   1034\u001b[0m     )\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\datasets\\builder.py:1124\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m   1120\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1123\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m-> 1124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split(split_generator, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs)\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   1127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1128\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1130\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m   1131\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\datasets\\builder.py:1884\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[1;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[0;32m   1882\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[1;32m-> 1884\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[0;32m   1885\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[0;32m   1886\u001b[0m     ):\n\u001b[0;32m   1887\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   1888\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\datasets\\builder.py:2040\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   2038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[0;32m   2039\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 2040\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2042\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"ChristophSchuhmann/wikipedia-en-chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cc3fc-b980-41b2-af0d-4eefe921a397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

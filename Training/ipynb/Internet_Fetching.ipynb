{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a379d2-7c82-44cd-afe7-ae1e749ec892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time\n",
    "from threading import Thread\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, col, arrays_zip\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "import math\n",
    "import random\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "chroma_client = chromadb.PersistentClient(path=\"./\")\n",
    "from pyspark.sql import functions as F\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e001675-1be8-4ce9-96f3-18fbe7264649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2a5784b-0f54-4cd7-9583-95f3f9262a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config(\"spark.driver.memory\", \"2g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"2g\") \\\n",
    "#     .appName(\"test\") \\\n",
    "#     .getOrCreate()\n",
    "# df = spark.createDataFrame([(1, 'Alice'), (2, 'Bob')], ['id', 'name'])\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4183c-ba9a-4713-a473-9652a6783eab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Context Based Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "080dcec0-f051-495e-8156-cfce3328e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = load_dataset(\"9wimu9/eli5_mult_answers_en_no_answer_in_context\") # in deep\n",
    "dataset_2 = load_dataset(\"mlxen/squad_1_1_smallcase_context\") #one word\n",
    "dataset_3 = load_dataset(\"nbtpj/multi-context-long-answer-dataset\") # in short\n",
    "dataset_8 = load_dataset(\"LasRuinasCirculares/sft_correct\")\n",
    "dataset_9 = load_dataset(\"robbiegwaldd/rephrase_train\") # all\n",
    "ds = load_dataset(\"addy88/nq-question-answeronly\")\n",
    "ds = load_dataset(\"iarfmoose/question_generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec2d2eb5-c9b1-4acd-97cc-020be5c8abcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'contexts', 'gold_answer'],\n",
       "        num_rows: 71236\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'contexts', 'gold_answer'],\n",
       "        num_rows: 7916\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "480eac2d-d4c0-453e-9d24-86ab9aefd0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = dataset_1[\"train\"][\"question\"]\n",
    "raw_context = dataset_1[\"train\"][\"contexts\"]\n",
    "modified_context = []\n",
    "for c in raw_context:\n",
    "    final_string = \"\"\n",
    "    for i in c:\n",
    "        final_string = final_string+\" \"+ i\n",
    "    modified_context.append(final_string)\n",
    "context = modified_context\n",
    "agent_2 = dataset_1[\"train\"][\"gold_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa842db5-a791-471f-b83c-bdeae45158e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1.extend(dataset_2[\"train\"][\"question\"])\n",
    "context.extend(dataset_2[\"train\"][\"context\"])\n",
    "agent_2.extend([i[\"text\"][0] for i in dataset_2[\"train\"][\"answers\"]])\n",
    "\n",
    "agent_1.extend([i.split(\"Question:\")[1].replace(\"Answer:\",\"\") for i in dataset_8[\"train\"][\"instruction\"]])\n",
    "context.extend([i.replace(\"Evidence:\",\"\").replace(\"**\",\"\").split(\"Question:\")[0].replace(\"\\n\",\"\") for i in dataset_8[\"train\"][\"instruction\"]])\n",
    "agent_2.extend(dataset_8[\"train\"][\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74b680-72dd-44c0-89af-0922731fdfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e107141a-a661-45a2-95d9-efae13f15f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelNames =[\"pubmed_qa_X_squad_num_channel_1_test\",\"pubmed_qa_X_squad_num_channel_1_train\",\"pubmed_qa_X_squad_num_channel_2_test\",\"pubmed_qa_X_squad_num_channel_2_train\",\"pubmed_qa_X_squad_num_channel_3_test\",\"pubmed_qa_X_squad_num_channel_3_train\",\"pubmed_qa_X_squad_num_channel_4_test\",\"pubmed_qa_X_squad_num_channel_4_train\",\"pubmed_qa_num_channel_1_test\",\"pubmed_qa_num_channel_1_train\",\"pubmed_qa_num_channel_2_test\",\"pubmed_qa_num_channel_2_train\",\"pubmed_qa_num_channel_3_test\",\"pubmed_qa_num_channel_3_train\",\"pubmed_qa_num_channel_4_test\",\"pubmed_qa_num_channel_4_train\",\"squad_num_channel_1_test\",\"squad_num_channel_1_train\",\"squad_num_channel_2_test\",\"squad_num_channel_2_train\",\"squad_num_channel_3_test\",\"squad_num_channel_3_train\",\"squad_num_channel_4_test\",\"squad_num_channel_4_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdf94cdd-d86a-4a5a-82b6-2397553294e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = []\n",
    "cont = []\n",
    "answer = []\n",
    "for channel in channelNames:\n",
    "    question.extend([i[0].split(\"<||||>\")[0] for i in dataset_3[channel][\"context\"]])\n",
    "    cont.extend([i[0].split(\"<||||>\")[1] for i in dataset_3[channel][\"context\"]])\n",
    "    answer.extend([i for i in dataset_3[channel][\"answer\"]])\n",
    "temp_df = pd.DataFrame()\n",
    "temp_df[\"question\"] = question\n",
    "temp_df[\"cont\"] = cont\n",
    "temp_df[\"answer\"] = answer\n",
    "df = temp_df.drop_duplicates(subset=['question','cont','answer'], keep='first')\n",
    "agent_1.extend(df[\"question\"].tolist())\n",
    "context.extend(df[\"cont\"].tolist())\n",
    "agent_2.extend(df[\"answer\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44ec1e40-ac8b-4813-997e-b1af74f2a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"bart-base-squad.end2end.amazon\",\"bart-base-squad.end2end.new_wiki\",\"bart-base-squad.end2end.nyt\",\"bart-base-squad.end2end.reddit\",\"bart-base-squad.multitask.amazon\",\"bart-base-squad.multitask.new_wiki\",\"bart-base-squad.multitask.nyt\",\"bart-base-squad.multitask.reddit\",\"bart-base-squad.pipeline.amazon\",\"bart-base-squad.pipeline.new_wiki\",\"bart-base-squad.pipeline.nyt\",\"bart-base-squad.pipeline.reddit\",\"bart-base-squad.qg_reference.amazon\",\"bart-base-squad.qg_reference.new_wiki\",\"bart-base-squad.qg_reference.nyt\",\"bart-base-squad.qg_reference.reddit\",\"bart-large-squad.end2end.amazon\",\"bart-large-squad.end2end.new_wiki\",\"bart-large-squad.end2end.nyt\",\"bart-large-squad.end2end.reddit\",\"bart-large-squad.multitask.amazon\",\"bart-large-squad.multitask.new_wiki\",\"bart-large-squad.multitask.nyt\",\"bart-large-squad.multitask.reddit\",\"bart-large-squad.pipeline.amazon\",\"bart-large-squad.pipeline.new_wiki\",\"bart-large-squad.pipeline.nyt\",\"bart-large-squad.pipeline.reddit\",\"bart-large-squad.qg_reference.amazon\",\"bart-large-squad.qg_reference.new_wiki\",\"bart-large-squad.qg_reference.nyt\",\"bart-large-squad.qg_reference.reddit\",\"t5-base-squad.end2end.amazon\",\"t5-base-squad.end2end.new_wiki\",\"t5-base-squad.end2end.nyt\",\"t5-base-squad.end2end.reddit\",\"t5-base-squad.multitask.amazon\",\"t5-base-squad.multitask.new_wiki\",\"t5-base-squad.multitask.nyt\",\"t5-base-squad.multitask.reddit\",\"t5-base-squad.pipeline.amazon\",\"t5-base-squad.pipeline.new_wiki\",\"t5-base-squad.pipeline.nyt\",\"t5-base-squad.pipeline.reddit\",\"t5-base-squad.qg_reference.amazon\",\"t5-base-squad.qg_reference.new_wiki\",\"t5-base-squad.qg_reference.nyt\",\"t5-base-squad.qg_reference.reddit\",\"t5-large-squad.end2end.amazon\",\"t5-large-squad.end2end.new_wiki\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ccd3579-5197-4e97-9d2d-520ca4b7ddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:00<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0,len(datasets))):\n",
    "    # print(f'\\rDataset Name: {dataset}', end='', flush=True)\n",
    "    ds = load_dataset(\"lmqg/qa_squadshifts_synthetic\", datasets[i])\n",
    "    agent_1.extend(i for i in ds[\"train\"][\"question\"])\n",
    "    context.extend(i for i in ds[\"train\"][\"context\"])\n",
    "    agent_2.extend(i[\"text\"][0] for i in ds[\"train\"][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17a8e6cd-5941-4807-901d-2b63795f09f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'original_snippet', 'rephrased_snippet'],\n",
       "        num_rows: 291032\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33933026-2739-45fb-b51c-4954b0d4710b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1469571"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4e96e63-3dcf-48b9-9c30-d6c38f9c866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291032/291032 [00:00<00:00, 369566.84it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "conte = []\n",
    "split_qa = [i.split(\"\\n\\n\") for i in dataset_9[\"train\"][\"rephrased_snippet\"]]\n",
    "cont = [i.replace(\"\\n\",\"\") for i in dataset_9[\"train\"][\"original_snippet\"]]\n",
    "for i in tqdm(range(0,len(split_qa))):\n",
    "    q = [];a = [];c = []\n",
    "    if \"\\nAnswer\" not in split_qa[i][0]:\n",
    "        if len(split_qa[i]) % 2 != 0:\n",
    "            split_qa[i] = split_qa[i][:-1]\n",
    "        for j in range(0,len(split_qa[i]),2):\n",
    "            q.append(split_qa[i][j])\n",
    "            a.append(split_qa[i][j+1])\n",
    "            c.append(cont[i])\n",
    "    else:\n",
    "        new_split = [j.split(\"\\n\") for j in split_qa[i]]\n",
    "        for j in range(0,len(new_split)):\n",
    "            if len(new_split[j]) %2 !=0:\n",
    "                continue\n",
    "            q.append(new_split[j][0].replace(\"Question:\",\"\"))\n",
    "            a.append(new_split[j][1].replace(\"Answer:\",\"\"))\n",
    "            c.append(cont[i])\n",
    "    answers.extend(a)\n",
    "    questions.extend(q)\n",
    "    conte.extend(c)\n",
    "    # print(len(answers),len(questions))\n",
    "    # print(split_qa[i])\n",
    "agent_1.extend(questions)\n",
    "context.extend(conte)\n",
    "agent_2.extend(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01a34bf1-b866-4fc3-9fab-b788012ca849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802143"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99e8d9de-e436-4064-a053-9f2b1a1f992f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802143"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9171f1e-c1f3-4a18-9f1f-124f3ba916a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802143"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0ef1d5f-ada4-4cd9-9fbb-a017a0a8180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame()\n",
    "temp_df[\"agent_1\"] = agent_1\n",
    "temp_df[\"context\"] = context\n",
    "temp_df[\"agent_2\"] = agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e09e709-eca8-4c04-9688-abcb852f7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv(\"../Data/CleanedDatasets/ContextBasedQuestions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b564f60-1d29-40ec-b265-e360fa3ef5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"../Data/CleanedDatasets/ContextBasedQuestions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e198ec01-a6fe-4902-8fea-9f314a08a239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_2</th>\n",
       "      <th>agent_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agent_2:These foods contain certain sugars tha...</td>\n",
       "      <td>&lt;context&gt;  Actually it is because most of peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agent_2:I'd answer the question but the defaul...</td>\n",
       "      <td>&lt;context&gt;  The decay of false vacuum is a fasc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agent_2:Essentially the events in Crimea threa...</td>\n",
       "      <td>&lt;context&gt;  National Sovereignty is a very big ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agent_2:To have as little protection as is saf...</td>\n",
       "      <td>&lt;context&gt;  Most helmets are pretty similar in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agent_2:Your eyes are magnificent sensors, and...</td>\n",
       "      <td>&lt;context&gt;  The reason is that a camera is basi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             agent_2  \\\n",
       "0  agent_2:These foods contain certain sugars tha...   \n",
       "1  agent_2:I'd answer the question but the defaul...   \n",
       "2  agent_2:Essentially the events in Crimea threa...   \n",
       "3  agent_2:To have as little protection as is saf...   \n",
       "4  agent_2:Your eyes are magnificent sensors, and...   \n",
       "\n",
       "                                             agent_1  \n",
       "0  <context>  Actually it is because most of peop...  \n",
       "1  <context>  The decay of false vacuum is a fasc...  \n",
       "2  <context>  National Sovereignty is a very big ...  \n",
       "3  <context>  Most helmets are pretty similar in ...  \n",
       "4  <context>  The reason is that a camera is basi...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6532a20-97ae-4c1a-902b-4d83dea290d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df[\"agent_2\"] = \"agent_2:\"+temp_df[\"agent_2\"]\n",
    "temp_df[\"agent_1\"] = \"<context> \" + temp_df[\"context\"] + \"agent_1:\"+temp_df[\"old_agent_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2508138-20d6-4649-9de9-848fc98da729",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.dropna(inplace = True)\n",
    "temp_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7aed2-0ec7-4dcb-a111-5113af5ca6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06f63fe5-368f-41c4-888a-e6dc0b81a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1799426 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1401 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 1799426/1799426 [45:44<00:00, 655.72it/s] \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "agent_1_extended = []\n",
    "agent_2_extended = []\n",
    "tokenizedLengthList = []\n",
    "idx = 0\n",
    "for input_text in tqdm(temp_df[\"agent_1\"].to_list()):\n",
    "    try:\n",
    "        input_tokenized = tokenizer(input_text,return_tensors='pt')\n",
    "        tokenizedLength = len(input_tokenized[\"input_ids\"][0])\n",
    "        tokenizedLengthList.append(tokenizedLength)\n",
    "        if tokenizedLength < 510:\n",
    "            agent_1_extended.append(input_text)\n",
    "            agent_2_extended.append(temp_df[\"agent_2\"][idx])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    idx = idx+1\n",
    "    # if tokenizedLength > 510 and tokenizedLength < 1500:\n",
    "        \n",
    "    #         num_of_loops = math.ceil(tokenizedLength/510)\n",
    "    #         slice_length = math.ceil(len(input_text)/num_of_loops)\n",
    "    #         for i in range(0,num_of_loops):\n",
    "    #             if i == 0:\n",
    "    #                 agent_1_extended.append(\"agent_1:\"+(input_text[i*slice_length:(i*slice_length)+slice_length])+\"<question>\"+temp_df[\"old_agent_1\"][idx])\n",
    "    #             else:\n",
    "    #                 agent_1_extended.append(\"agent_1:\"+\"<context>\"+(input_text[(i*slice_length)-50:(i*slice_length)+slice_length])+\"<question>\"+temp_df[\"old_agent_1\"][idx])\n",
    "    \n",
    "    # else:\n",
    "    #     agent_1_extended.append(\"agent_1:\"+input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43fbe6ac-67eb-46cb-ac4c-a6ccea026b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400831"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_1_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca6c8887-3b4a-4876-920c-00f8aa9e7baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400831"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_2_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84788807-f494-4ced-bbd9-31cee65eb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame()\n",
    "temp_df[\"agent_2\"] =agent_2_extended\n",
    "temp_df[\"agent_1\"] =agent_1_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cd8236-afd2-4c82-b885-c400b6a58fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<context>  National Sovereignty is a very big deal so anytime a country (especially a country with nukes) starts to reject previously agreed upon borders it is a big deal.\\n\\nThe reason you should care is that Russia has been for quite sometime trying to maintain some degree of influence over several former Soviet republics.  As some of these states move away from Russia, it is going to cause conflict and potentially a full blown war. likely to evolve into nothing\\n\\ncrimea was granted to ukraine as a restitution gift and well over half the population is in favour of annexation\\n\\nthey already speak russian and maintain russian culture\\n\\nputin is a total dick but once this blows over they will pay lower taxes to an equally corrupt government and receive additional social benefits\\n\\nyeah, its a huge problem that a part of a country is essentially being taken over but it hardly sets precedent for speculations of a third world war\\n\\nagent_1:what is the big deal about russia invading ukraine and taking over crimea, and what do i, as a us citizen have to worry about it?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_1_extended[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24687bb3-c02b-40dc-be77-d953121909ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv(\"../Data/CleanedDatasets/ContextBasedQuestions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cfbb514-ada6-4fae-a7ac-67bc8db4f902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<context> 5 best Java seeds for Minecraft 1.19 updateThe 1.19 update for Minecraft introduced many new features. \\xa0Two major biomes, Deep Dark and Mangrove Swamp, have been added to the game and are quickly becoming quite popular.The two biomes are arguably the biggest addition to the game. \\xa0Deep Dark and Mangrove Swamp both provide reasons to visit them, but there's only one problem: they can be difficult to find.Minecraft seeds to try in Java Edition version 1.19Multiple Blue Rings\\xa0Note: The 1.18 update introduced seed universality, so seeds will work on both versions with minimal differences. \\xa0The main differences are when moving from Java to Bedrock, but not vice versa.The ancient city in 1.19 is a new but extremely rare structure. \\xa0It's a challenge to trace a regular Deep Dark with just the Skulk and the Warden. \\xa0Most of those found do not even contain any ancient cities.Seed: -457009213479927390This seed puts players into an amazing spawn. \\xa0Gamers come right next to a village, which is always good for getting loot and other items.Seed: -6709148406763899126Good loot can be a dealbreaker for many Minecraft gamers. \\xa0If a particular seed has good loot, such as diamonds in a blacksmith's chest, players are more willing to try it. \\xa0This seed does not necessarily have that, but there is something even more rare in it.Seed: -2110863992403414331The seeds are devoid of a mangrove swamp village because they do not exist. \\xa0However, this village originates from swamps, so it matters.Seed: 1450778142214593647Mangrove swamps are a big part of the Minecraft 1.19 update, and players want to find it. \\xa0Luckily, this Java seed gives gamers that opportunity. \\xa0It also has a very unique biome generation.Seed: 6705098208300174216agent_1: What are the two major biomes in Minecraft 1.19 and how do they differ from Deep Dark and Mangrove Swamp?\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df[\"agent_1\"][1400700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2e83014-4dd7-4388-a9f4-4e48c932c981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agent_2: In Minecraft 1.19, there are two new biomes added: \"Deep Dark\" and \"Mangrove Swamp\". The Deep Dark biome provides reasons to visit it but can be difficult to find due to its location near a swamp. Meanwhile, the Mangrove Swamp biome has only one problem - it can be difficult to find as well. Minecraft seeds like -457009213479927390, -6709148406763899126, and -2110863992403414331 can be used to trace a Deep Dark with just the Skulk and Warden. These seeds contain good loot but do not necessarily have Mangrove Swamp as they are located in swamps instead of in the swamp biome. The seeds -2110863992403414331 and 1450778142214593647 can be used to find Mangrove Swamp by using a village as an alternative spawn point. The seeds -6709148406763899126 and 6705'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df[\"agent_2\"][1400700]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362406fe-3022-4d03-987e-bf2e1a6ac1ac",
   "metadata": {},
   "source": [
    "### Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e488c1af-51a5-4674-a23d-b8dc88db0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = '../Saved_Models/Sky/fine-tuned-bert-sentiment_2024_10_05_0'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddfc833-c36c-4103-a16a-cc8a5660e49f",
   "metadata": {},
   "source": [
    "#### Normal Search Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9963382-6a6f-4718-977f-b1bd3a81dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is the universe created?\"\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9d94dbc-f4d6-43ed-804e-60cf322d2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "def get_google_search_links(query):\n",
    "    return [link for link in search(query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf38a1b-1a84-49e7-a7c2-b93b6f7a0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_latex(text):\n",
    "    # Remove LaTeX commands like {\\\\displaystyle}, {\\\\text{}}, etc.\n",
    "    cleaned_text = re.sub(r'\\\\displaystyle|\\\\text\\{.*?\\}', '', text)\n",
    "    \n",
    "    # Remove any LaTeX curly braces and unnecessary whitespaces\n",
    "    cleaned_text = re.sub(r'\\\\[a-z]+|{|}', '', cleaned_text)\n",
    "    \n",
    "    # Replace LaTeX-specific representations like \\\\dots with their equivalent\n",
    "    cleaned_text = re.sub(r'\\\\dots', '...', cleaned_text)\n",
    "    \n",
    "    # Remove multiple spaces introduced by LaTeX removal\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2576fb34-3e2f-4892-b6f2-c5ea548e65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_links = get_google_search_links(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f319701-0ec5-466d-875d-d0c4a6f53d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.reddit.com/r/AskPhysics/comments/1cezc6i/why_is_our_universe_3d/',\n",
       " 'https://www.quora.com/What-is-the-universe-How-was-it-created-and-when',\n",
       " 'https://www.cfa.harvard.edu/big-questions/what-universe-made',\n",
       " 'https://www.quora.com/Did-the-universe-exist-in-three-dimensions-before-the-Big-Bang-Was-there-any-need-for-a-three-dimensional-universe-before-the-expansion-of-the-universe',\n",
       " 'https://physics.stackexchange.com/questions/313392/why-a-3d-universe',\n",
       " 'https://www.esa.int/Science_Exploration/Space_Science/Extreme_space/What_is_the_Universe_made_of',\n",
       " 'https://www.physicsforums.com/threads/what-made-the-universe-have-3-dimensions-instead-of-some-other-number.430304/',\n",
       " 'https://www.astronomy.com/science/what-shape-is-the-universe/',\n",
       " 'https://en.wikipedia.org/wiki/Expansion_of_the_universe',\n",
       " 'https://www.cnn.com/2020/07/22/world/universe-3d-map-intl-hnk-scn-scli/index.html']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c86d4f3-edb3-47d4-b03d-b68dd637592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadWithReturnValue(Thread):\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs={}, Verbose=None):\n",
    "        Thread.__init__(self, group, target, name, args, kwargs)\n",
    "        self._return = None\n",
    "\n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args,\n",
    "                                                **self._kwargs)\n",
    "    def join(self, *args):\n",
    "        Thread.join(self, *args)\n",
    "        return self._return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "082e8261-7f77-43db-bb06-c37bdf97251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_mining(website):\n",
    "    filtered_content = \"\"\n",
    "    if (\".gov\" not in website) and (\"linkedin.com\" not in website) and (\"reddit.com\" not in website):\n",
    "        URL = website\n",
    "        r = requests.get(URL) \n",
    "        soup = BeautifulSoup(r.content, 'html5lib')\n",
    "        for tag in soup(['nav', 'header', 'footer', 'script', 'style', 'aside']):\n",
    "            tag.decompose()\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li','strong']):\n",
    "            filtered_content = filtered_content+tag.get_text()\n",
    "    return filtered_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d68e62-43fd-4a65-92ee-0ec147b6bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# # spark = SparkSession.builder.appName(\"WebScrapingWithPySpark\").getOrCreate()\n",
    "# threads_list = [ThreadWithReturnValue(target=data_mining, args=(website,)) for website in website_links[:5]]\n",
    "# [thread.start() for thread in threads_list]\n",
    "# fetched_data = [thread.join() for thread in threads_list]\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21937f70-d5e8-4699-8aee-5a4acd65aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(cleaned_data):\n",
    "    ids_list = []\n",
    "    final_chunks = []\n",
    "    random_number = random.randint(0,10000000000)\n",
    "    loop = math.ceil(len(cleaned_data)/2000)\n",
    "    for i in range(0,loop):\n",
    "        if i ==0:\n",
    "            final_chunks.append(cleaned_data[(i*2000):(i+1)*2000])\n",
    "        else:\n",
    "            final_chunks.append(cleaned_data[(i*2000)-500:(i+1)*2000])\n",
    "        ids_list.append(str(random_number+(i/50)))\n",
    "    return (final_chunks,ids_list)\n",
    "    # return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3913f4c0-37f2-4856-ae71-ec90ec86f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.appName(\"WebScrapingWithPySpark\").getOrCreate()\n",
    "threads_list = [ThreadWithReturnValue(target=data_mining, args=(website,)) for website in website_links]\n",
    "[thread.start() for thread in threads_list]\n",
    "fetched_data = [thread.join() for thread in threads_list]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .appName(\"WebScrapingWithPySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "cleaned_data_udf = udf(clean_latex, StringType())\n",
    "# create_chunks_udf = udf(create_chunks, ArrayType(StringType()), ArrayType(StringType()))\n",
    "schema = StructType([\n",
    "    StructField(\"chunks\", ArrayType(StringType())),\n",
    "    StructField(\"ids\", ArrayType(StringType()))\n",
    "])\n",
    "create_chunks_udf = udf(create_chunks, schema)\n",
    "\n",
    "temp_df = list(zip(website_links, fetched_data))\n",
    "uncleaned_df = spark.createDataFrame(temp_df, [\"url\", \"uncleaned_data\"])\n",
    "cleaned_df = uncleaned_df.withColumn(\"cleaned_data\", cleaned_data_udf(\"uncleaned_data\"))\n",
    "clean_df = cleaned_df.withColumn(\"processed\", create_chunks_udf(\"cleaned_data\"))\n",
    "\n",
    "clean_df = clean_df.select(\n",
    "    F.col('processed.chunks').alias(\"chunks\"),\n",
    "    F.col('processed.ids').alias(\"ids\")\n",
    ")\n",
    "\n",
    "chunks_df = clean_df.withColumn(\"zipped\", arrays_zip(\"chunks\", \"ids\")) \\\n",
    "       .withColumn(\"exploded\", explode(\"zipped\")) \\\n",
    "       .select(clean_df.chunks.alias(\"chunks_array\"), \"exploded.chunks\", \"exploded.ids\")\n",
    "chunks_df = chunks_df.drop('chunks_array')\n",
    "\n",
    "# pandas_df = chunks_df.toPandas()\n",
    "# conn = sqlite3.connect('chroma.sqlite3')  # 'example.db' is the SQLite database file\n",
    "# pandas_df.to_sql('table_name', conn, if_exists='replace', index=False)\n",
    "# conn.close()\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb47bee1-c661-420b-909e-026931c08568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|              chunks|          ids|\n",
      "+--------------------+-------------+\n",
      "|Something went wr...| 8895532017.0|\n",
      "|What is the unive...| 9004580368.0|\n",
      "|th gravity throug...|9004580368.02|\n",
      "|earching for answ...|9004580368.04|\n",
      "|ble demonstrated ...|9004580368.06|\n",
      "|Something went wr...| 6627677278.0|\n",
      "|TeamsTeamsQ&A for...| 2874668329.0|\n",
      "|ed down\" to the f...|2874668329.02|\n",
      "|prevent orbital d...|2874668329.04|\n",
      "|The Universe is t...| 8623402453.0|\n",
      "| clusters of gala...|8623402453.02|\n",
      "|Classical Physics...| 2486616701.0|\n",
      "|As Princeton Univ...| 8113499238.0|\n",
      "|en encounter in e...|8113499238.02|\n",
      "|ty of Rome and hi...|8113499238.04|\n",
      "|ig Bang — also su...|8113499238.06|\n",
      "|cience Dark Energ...|8113499238.08|\n",
      "|Big Bang · Univer...| 8619427526.0|\n",
      "|e from the observ...|8619427526.02|\n",
      "|mological model, ...|8619427526.04|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time: 1.123382806777954 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "chunks_df.show()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdfc18e9-5fc9-4c09-b172-0ad28fb7c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09bd28d-c6d1-47b1-ad6e-cf2380d32640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7d367ba-75fa-4f23-8332-02a75407081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.upsert(\n",
    "    documents=final_chunks,\n",
    "    ids=ids_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db8fe6dd-67a3-4d57-9452-980ed3659b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.07861828804016113 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query], # Chroma will embed this for you\n",
    "    n_results=2 # how many results to return\n",
    ")\n",
    "# print(results)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total time taken\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "741fb75b-c061-4ee6-bb47-821d0284486f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7082719453848038, 0.7667292356491089]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['distances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed059ac8-a779-4f76-ac45-e3d950b84a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agent_2:Yes, he has a Canadian passport.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"<context>{}agent_1:{}\".format(results[\"documents\"][0][0],query), return_tensors='pt',max_length=1024, truncation=True).to(device)\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=512, \n",
    "    num_beams=10, \n",
    "    early_stopping=True, \n",
    "    no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
    "    num_return_sequences=1,  # Number of sequences to return\n",
    "    temperature=0.7,  # Sampling temperature\n",
    "    top_k=50,  # Top-K sampling\n",
    "    top_p=0.9  # Top-p (nucleus) sampling\n",
    ")\n",
    "resp = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "364eb77e-d642-494e-8572-bc8de7314424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output: agentagent!-->omes paradise, swanky Mauritius bungalow, and a stunning villa in Goa. how Akshay transformed from a humble beginning to owning some of the world’s most luxurious homes. With exclusive insights into his glamorous residences and the stunning interiors crafted by his wife Twinkle Khanna, get ready to be amazed by the opulence and grandeur that define the Khiladi’s lifestyle! of Akshay Kumar & Home Address of Akshay Kumar was Home Address Akshay Kumar was born on the 9th of September 1967. He was born in Amritsar, Punjab, India and moved to Delhi where he spent most of his childhood. When his father retired from the army to join UNICEF he moved to Mumbai. Rajiv Hari Om Bhatia made his way into Hindi cinema in the early 1990s, after a series of flops came the breakthrough role in Khiladi directed by Abas-Mastan established Akshay Kumar as an action hero. Akshay Kumar adopted the professional name of Akshay Kumar before stepping into movies. His charming smile and lean physique made him a ladies' man. Even today at 52, he is of the fittest and sexiest actors in Bollywood. He Service Guarantee Or Painting Free Get a rental agreement with doorstep delivery Find the BEST deals and get unbelievable discountsCOUNTS directly from builders! 5-Star rated painters, premium paints and services at the BEST PRICES! a commercial superstar, Akshay Kumar has made many content-rich movies in recent times, Rustom, Padman, and Kesari are a few of the fantastic movies that propelled him into newfound popularity. He is known for his discipline and punctuality, as well as for being a prankster on sets. He does not party hard nor do in indulge in alcohol or drugs, so what does he spend his money on? Akshay Kumar House Locations Akshay Kumar Locations Akshfront Juhu bungalow, A home in Mauritius, A in Maurit, Four more flats in Andheri some of the smart real-estate investments he has made over the years. Akshay has invested in properties globally and owns some of the most expensive homes in the world. Chef tostar Chef Superstariv Bhatia as a in a small in Bangkok before the shiny of B\n",
      "Confidence Score: 0.9339369535446167\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(\"<context>{}agent_1:{}\".format(results[\"documents\"][0][0],query), return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "\n",
    "# Forward pass through the model to get the logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get the predicted token ids from the logits (the most likely tokens)\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Calculate the probabilities (softmax) for the predicted tokens\n",
    "probabilities = softmax(logits, dim=-1)\n",
    "\n",
    "# Get the probabilities of the predicted tokens\n",
    "predicted_probabilities = probabilities.gather(2, predicted_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# Calculate the confidence score as the average probability of the predicted tokens\n",
    "confidence_score = predicted_probabilities.mean().item()\n",
    "\n",
    "print(\"Generated Output:\", tokenizer.decode(predicted_ids[0], skip_special_tokens=True).replace(\"\\n\",\"\"))\n",
    "print(\"Confidence Score:\", confidence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0105b-adfe-44ec-9801-724f68dfad6c",
   "metadata": {},
   "source": [
    "# Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d0717a22-14d5-4e12-a41d-b9f13ea725d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('chroma.sqlite3') \n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "30f76bec-da85-450f-83da-85887aeb5a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('migrations',), ('embeddings_queue',), ('embeddings_queue_config',), ('collection_metadata',), ('segment_metadata',), ('tenants',), ('databases',), ('collections',), ('maintenance_log',), ('segments',), ('embeddings',), ('embedding_metadata',), ('max_seq_id',), ('embedding_fulltext_search',), ('embedding_fulltext_search_data',), ('embedding_fulltext_search_idx',), ('embedding_fulltext_search_content',), ('embedding_fulltext_search_docsize',), ('embedding_fulltext_search_config',), ('table_name',)]\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"SELECT name FROM sqlite_master  \n",
    "  WHERE type='table';\"\"\"\n",
    "cursor.execute(sql_query)\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e68410c5-f82f-435b-a6eb-e3e2dfa1d0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x19b151390a0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = \"\"\"SELECT * FROM collections;\"\"\"\n",
    "cursor.execute(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3adf4fb-96b8-402b-bb97-216572a4eda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1c13d8d4-830b-4def-928f-fb4fc943daa2', 'my_collection', 384, '00000000-0000-0000-0000-000000000000', '{\"hnsw_configuration\": {\"space\": \"l2\", \"ef_construction\": 100, \"ef_search\": 10, \"num_threads\": 28, \"M\": 16, \"resize_factor\": 1.2, \"batch_size\": 100, \"sync_threshold\": 1000, \"_type\": \"HNSWConfigurationInternal\"}, \"_type\": \"CollectionConfigurationInternal\"}')]\n"
     ]
    }
   ],
   "source": [
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b89d3-b08c-4269-940e-19cf0d9c189e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

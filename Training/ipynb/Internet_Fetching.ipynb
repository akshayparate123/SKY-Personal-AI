{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a379d2-7c82-44cd-afe7-ae1e749ec892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time\n",
    "from threading import Thread\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "import math\n",
    "import random\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "chroma_client = chromadb.PersistentClient(path=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e001675-1be8-4ce9-96f3-18fbe7264649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a5784b-0f54-4cd7-9583-95f3f9262a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config(\"spark.driver.memory\", \"2g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"2g\") \\\n",
    "#     .appName(\"test\") \\\n",
    "#     .getOrCreate()\n",
    "# df = spark.createDataFrame([(1, 'Alice'), (2, 'Bob')], ['id', 'name'])\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4183c-ba9a-4713-a473-9652a6783eab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Context Based Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "080dcec0-f051-495e-8156-cfce3328e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = load_dataset(\"9wimu9/eli5_mult_answers_en_no_answer_in_context\") # in deep\n",
    "dataset_2 = load_dataset(\"mlxen/squad_1_1_smallcase_context\") #one word\n",
    "dataset_3 = load_dataset(\"nbtpj/multi-context-long-answer-dataset\") # in short\n",
    "dataset_8 = load_dataset(\"LasRuinasCirculares/sft_correct\")\n",
    "dataset_9 = load_dataset(\"robbiegwaldd/rephrase_train\") # all\n",
    "ds = load_dataset(\"addy88/nq-question-answeronly\")\n",
    "ds = load_dataset(\"iarfmoose/question_generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec2d2eb5-c9b1-4acd-97cc-020be5c8abcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'contexts', 'gold_answer'],\n",
       "        num_rows: 71236\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'contexts', 'gold_answer'],\n",
       "        num_rows: 7916\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "480eac2d-d4c0-453e-9d24-86ab9aefd0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = dataset_1[\"train\"][\"question\"]\n",
    "raw_context = dataset_1[\"train\"][\"contexts\"]\n",
    "modified_context = []\n",
    "for c in raw_context:\n",
    "    final_string = \"\"\n",
    "    for i in c:\n",
    "        final_string = final_string+\" \"+ i\n",
    "    modified_context.append(final_string)\n",
    "context = modified_context\n",
    "agent_2 = dataset_1[\"train\"][\"gold_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa842db5-a791-471f-b83c-bdeae45158e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1.extend(dataset_2[\"train\"][\"question\"])\n",
    "context.extend(dataset_2[\"train\"][\"context\"])\n",
    "agent_2.extend([i[\"text\"][0] for i in dataset_2[\"train\"][\"answers\"]])\n",
    "\n",
    "agent_1.extend([i.split(\"Question:\")[1].replace(\"Answer:\",\"\") for i in dataset_8[\"train\"][\"instruction\"]])\n",
    "context.extend([i.replace(\"Evidence:\",\"\").replace(\"**\",\"\").split(\"Question:\")[0].replace(\"\\n\",\"\") for i in dataset_8[\"train\"][\"instruction\"]])\n",
    "agent_2.extend(dataset_8[\"train\"][\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74b680-72dd-44c0-89af-0922731fdfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e107141a-a661-45a2-95d9-efae13f15f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelNames =[\"pubmed_qa_X_squad_num_channel_1_test\",\"pubmed_qa_X_squad_num_channel_1_train\",\"pubmed_qa_X_squad_num_channel_2_test\",\"pubmed_qa_X_squad_num_channel_2_train\",\"pubmed_qa_X_squad_num_channel_3_test\",\"pubmed_qa_X_squad_num_channel_3_train\",\"pubmed_qa_X_squad_num_channel_4_test\",\"pubmed_qa_X_squad_num_channel_4_train\",\"pubmed_qa_num_channel_1_test\",\"pubmed_qa_num_channel_1_train\",\"pubmed_qa_num_channel_2_test\",\"pubmed_qa_num_channel_2_train\",\"pubmed_qa_num_channel_3_test\",\"pubmed_qa_num_channel_3_train\",\"pubmed_qa_num_channel_4_test\",\"pubmed_qa_num_channel_4_train\",\"squad_num_channel_1_test\",\"squad_num_channel_1_train\",\"squad_num_channel_2_test\",\"squad_num_channel_2_train\",\"squad_num_channel_3_test\",\"squad_num_channel_3_train\",\"squad_num_channel_4_test\",\"squad_num_channel_4_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdf94cdd-d86a-4a5a-82b6-2397553294e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = []\n",
    "cont = []\n",
    "answer = []\n",
    "for channel in channelNames:\n",
    "    question.extend([i[0].split(\"<||||>\")[0] for i in dataset_3[channel][\"context\"]])\n",
    "    cont.extend([i[0].split(\"<||||>\")[1] for i in dataset_3[channel][\"context\"]])\n",
    "    answer.extend([i for i in dataset_3[channel][\"answer\"]])\n",
    "temp_df = pd.DataFrame()\n",
    "temp_df[\"question\"] = question\n",
    "temp_df[\"cont\"] = cont\n",
    "temp_df[\"answer\"] = answer\n",
    "df = temp_df.drop_duplicates(subset=['question','cont','answer'], keep='first')\n",
    "agent_1.extend(df[\"question\"].tolist())\n",
    "context.extend(df[\"cont\"].tolist())\n",
    "agent_2.extend(df[\"answer\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44ec1e40-ac8b-4813-997e-b1af74f2a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"bart-base-squad.end2end.amazon\",\"bart-base-squad.end2end.new_wiki\",\"bart-base-squad.end2end.nyt\",\"bart-base-squad.end2end.reddit\",\"bart-base-squad.multitask.amazon\",\"bart-base-squad.multitask.new_wiki\",\"bart-base-squad.multitask.nyt\",\"bart-base-squad.multitask.reddit\",\"bart-base-squad.pipeline.amazon\",\"bart-base-squad.pipeline.new_wiki\",\"bart-base-squad.pipeline.nyt\",\"bart-base-squad.pipeline.reddit\",\"bart-base-squad.qg_reference.amazon\",\"bart-base-squad.qg_reference.new_wiki\",\"bart-base-squad.qg_reference.nyt\",\"bart-base-squad.qg_reference.reddit\",\"bart-large-squad.end2end.amazon\",\"bart-large-squad.end2end.new_wiki\",\"bart-large-squad.end2end.nyt\",\"bart-large-squad.end2end.reddit\",\"bart-large-squad.multitask.amazon\",\"bart-large-squad.multitask.new_wiki\",\"bart-large-squad.multitask.nyt\",\"bart-large-squad.multitask.reddit\",\"bart-large-squad.pipeline.amazon\",\"bart-large-squad.pipeline.new_wiki\",\"bart-large-squad.pipeline.nyt\",\"bart-large-squad.pipeline.reddit\",\"bart-large-squad.qg_reference.amazon\",\"bart-large-squad.qg_reference.new_wiki\",\"bart-large-squad.qg_reference.nyt\",\"bart-large-squad.qg_reference.reddit\",\"t5-base-squad.end2end.amazon\",\"t5-base-squad.end2end.new_wiki\",\"t5-base-squad.end2end.nyt\",\"t5-base-squad.end2end.reddit\",\"t5-base-squad.multitask.amazon\",\"t5-base-squad.multitask.new_wiki\",\"t5-base-squad.multitask.nyt\",\"t5-base-squad.multitask.reddit\",\"t5-base-squad.pipeline.amazon\",\"t5-base-squad.pipeline.new_wiki\",\"t5-base-squad.pipeline.nyt\",\"t5-base-squad.pipeline.reddit\",\"t5-base-squad.qg_reference.amazon\",\"t5-base-squad.qg_reference.new_wiki\",\"t5-base-squad.qg_reference.nyt\",\"t5-base-squad.qg_reference.reddit\",\"t5-large-squad.end2end.amazon\",\"t5-large-squad.end2end.new_wiki\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ccd3579-5197-4e97-9d2d-520ca4b7ddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:00<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0,len(datasets))):\n",
    "    # print(f'\\rDataset Name: {dataset}', end='', flush=True)\n",
    "    ds = load_dataset(\"lmqg/qa_squadshifts_synthetic\", datasets[i])\n",
    "    agent_1.extend(i for i in ds[\"train\"][\"question\"])\n",
    "    context.extend(i for i in ds[\"train\"][\"context\"])\n",
    "    agent_2.extend(i[\"text\"][0] for i in ds[\"train\"][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17a8e6cd-5941-4807-901d-2b63795f09f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'original_snippet', 'rephrased_snippet'],\n",
       "        num_rows: 291032\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33933026-2739-45fb-b51c-4954b0d4710b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1469571"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4e96e63-3dcf-48b9-9c30-d6c38f9c866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291032/291032 [00:00<00:00, 369566.84it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "conte = []\n",
    "split_qa = [i.split(\"\\n\\n\") for i in dataset_9[\"train\"][\"rephrased_snippet\"]]\n",
    "cont = [i.replace(\"\\n\",\"\") for i in dataset_9[\"train\"][\"original_snippet\"]]\n",
    "for i in tqdm(range(0,len(split_qa))):\n",
    "    q = [];a = [];c = []\n",
    "    if \"\\nAnswer\" not in split_qa[i][0]:\n",
    "        if len(split_qa[i]) % 2 != 0:\n",
    "            split_qa[i] = split_qa[i][:-1]\n",
    "        for j in range(0,len(split_qa[i]),2):\n",
    "            q.append(split_qa[i][j])\n",
    "            a.append(split_qa[i][j+1])\n",
    "            c.append(cont[i])\n",
    "    else:\n",
    "        new_split = [j.split(\"\\n\") for j in split_qa[i]]\n",
    "        for j in range(0,len(new_split)):\n",
    "            if len(new_split[j]) %2 !=0:\n",
    "                continue\n",
    "            q.append(new_split[j][0].replace(\"Question:\",\"\"))\n",
    "            a.append(new_split[j][1].replace(\"Answer:\",\"\"))\n",
    "            c.append(cont[i])\n",
    "    answers.extend(a)\n",
    "    questions.extend(q)\n",
    "    conte.extend(c)\n",
    "    # print(len(answers),len(questions))\n",
    "    # print(split_qa[i])\n",
    "agent_1.extend(questions)\n",
    "context.extend(conte)\n",
    "agent_2.extend(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01a34bf1-b866-4fc3-9fab-b788012ca849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802143"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99e8d9de-e436-4064-a053-9f2b1a1f992f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802143"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9171f1e-c1f3-4a18-9f1f-124f3ba916a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802143"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0ef1d5f-ada4-4cd9-9fbb-a017a0a8180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame()\n",
    "temp_df[\"agent_1\"] = agent_1\n",
    "temp_df[\"context\"] = context\n",
    "temp_df[\"agent_2\"] = agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e09e709-eca8-4c04-9688-abcb852f7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv(\"../Data/CleanedDatasets/ContextBasedQuestions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b564f60-1d29-40ec-b265-e360fa3ef5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"../Data/CleanedDatasets/ContextBasedQuestions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e198ec01-a6fe-4902-8fea-9f314a08a239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_2</th>\n",
       "      <th>agent_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agent_2:These foods contain certain sugars tha...</td>\n",
       "      <td>&lt;context&gt;  Actually it is because most of peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agent_2:I'd answer the question but the defaul...</td>\n",
       "      <td>&lt;context&gt;  The decay of false vacuum is a fasc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agent_2:Essentially the events in Crimea threa...</td>\n",
       "      <td>&lt;context&gt;  National Sovereignty is a very big ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agent_2:To have as little protection as is saf...</td>\n",
       "      <td>&lt;context&gt;  Most helmets are pretty similar in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agent_2:Your eyes are magnificent sensors, and...</td>\n",
       "      <td>&lt;context&gt;  The reason is that a camera is basi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             agent_2  \\\n",
       "0  agent_2:These foods contain certain sugars tha...   \n",
       "1  agent_2:I'd answer the question but the defaul...   \n",
       "2  agent_2:Essentially the events in Crimea threa...   \n",
       "3  agent_2:To have as little protection as is saf...   \n",
       "4  agent_2:Your eyes are magnificent sensors, and...   \n",
       "\n",
       "                                             agent_1  \n",
       "0  <context>  Actually it is because most of peop...  \n",
       "1  <context>  The decay of false vacuum is a fasc...  \n",
       "2  <context>  National Sovereignty is a very big ...  \n",
       "3  <context>  Most helmets are pretty similar in ...  \n",
       "4  <context>  The reason is that a camera is basi...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6532a20-97ae-4c1a-902b-4d83dea290d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df[\"agent_2\"] = \"agent_2:\"+temp_df[\"agent_2\"]\n",
    "temp_df[\"agent_1\"] = \"<context> \" + temp_df[\"context\"] + \"agent_1:\"+temp_df[\"old_agent_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2508138-20d6-4649-9de9-848fc98da729",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.dropna(inplace = True)\n",
    "temp_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7aed2-0ec7-4dcb-a111-5113af5ca6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06f63fe5-368f-41c4-888a-e6dc0b81a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1799426 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1401 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 1799426/1799426 [45:44<00:00, 655.72it/s] \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "agent_1_extended = []\n",
    "agent_2_extended = []\n",
    "tokenizedLengthList = []\n",
    "idx = 0\n",
    "for input_text in tqdm(temp_df[\"agent_1\"].to_list()):\n",
    "    try:\n",
    "        input_tokenized = tokenizer(input_text,return_tensors='pt')\n",
    "        tokenizedLength = len(input_tokenized[\"input_ids\"][0])\n",
    "        tokenizedLengthList.append(tokenizedLength)\n",
    "        if tokenizedLength < 510:\n",
    "            agent_1_extended.append(input_text)\n",
    "            agent_2_extended.append(temp_df[\"agent_2\"][idx])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    idx = idx+1\n",
    "    # if tokenizedLength > 510 and tokenizedLength < 1500:\n",
    "        \n",
    "    #         num_of_loops = math.ceil(tokenizedLength/510)\n",
    "    #         slice_length = math.ceil(len(input_text)/num_of_loops)\n",
    "    #         for i in range(0,num_of_loops):\n",
    "    #             if i == 0:\n",
    "    #                 agent_1_extended.append(\"agent_1:\"+(input_text[i*slice_length:(i*slice_length)+slice_length])+\"<question>\"+temp_df[\"old_agent_1\"][idx])\n",
    "    #             else:\n",
    "    #                 agent_1_extended.append(\"agent_1:\"+\"<context>\"+(input_text[(i*slice_length)-50:(i*slice_length)+slice_length])+\"<question>\"+temp_df[\"old_agent_1\"][idx])\n",
    "    \n",
    "    # else:\n",
    "    #     agent_1_extended.append(\"agent_1:\"+input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43fbe6ac-67eb-46cb-ac4c-a6ccea026b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400831"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_1_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca6c8887-3b4a-4876-920c-00f8aa9e7baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400831"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent_2_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84788807-f494-4ced-bbd9-31cee65eb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame()\n",
    "temp_df[\"agent_2\"] =agent_2_extended\n",
    "temp_df[\"agent_1\"] =agent_1_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cd8236-afd2-4c82-b885-c400b6a58fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<context>  National Sovereignty is a very big deal so anytime a country (especially a country with nukes) starts to reject previously agreed upon borders it is a big deal.\\n\\nThe reason you should care is that Russia has been for quite sometime trying to maintain some degree of influence over several former Soviet republics.  As some of these states move away from Russia, it is going to cause conflict and potentially a full blown war. likely to evolve into nothing\\n\\ncrimea was granted to ukraine as a restitution gift and well over half the population is in favour of annexation\\n\\nthey already speak russian and maintain russian culture\\n\\nputin is a total dick but once this blows over they will pay lower taxes to an equally corrupt government and receive additional social benefits\\n\\nyeah, its a huge problem that a part of a country is essentially being taken over but it hardly sets precedent for speculations of a third world war\\n\\nagent_1:what is the big deal about russia invading ukraine and taking over crimea, and what do i, as a us citizen have to worry about it?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_1_extended[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24687bb3-c02b-40dc-be77-d953121909ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv(\"../Data/CleanedDatasets/ContextBasedQuestions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cfbb514-ada6-4fae-a7ac-67bc8db4f902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<context> 5 best Java seeds for Minecraft 1.19 updateThe 1.19 update for Minecraft introduced many new features. \\xa0Two major biomes, Deep Dark and Mangrove Swamp, have been added to the game and are quickly becoming quite popular.The two biomes are arguably the biggest addition to the game. \\xa0Deep Dark and Mangrove Swamp both provide reasons to visit them, but there's only one problem: they can be difficult to find.Minecraft seeds to try in Java Edition version 1.19Multiple Blue Rings\\xa0Note: The 1.18 update introduced seed universality, so seeds will work on both versions with minimal differences. \\xa0The main differences are when moving from Java to Bedrock, but not vice versa.The ancient city in 1.19 is a new but extremely rare structure. \\xa0It's a challenge to trace a regular Deep Dark with just the Skulk and the Warden. \\xa0Most of those found do not even contain any ancient cities.Seed: -457009213479927390This seed puts players into an amazing spawn. \\xa0Gamers come right next to a village, which is always good for getting loot and other items.Seed: -6709148406763899126Good loot can be a dealbreaker for many Minecraft gamers. \\xa0If a particular seed has good loot, such as diamonds in a blacksmith's chest, players are more willing to try it. \\xa0This seed does not necessarily have that, but there is something even more rare in it.Seed: -2110863992403414331The seeds are devoid of a mangrove swamp village because they do not exist. \\xa0However, this village originates from swamps, so it matters.Seed: 1450778142214593647Mangrove swamps are a big part of the Minecraft 1.19 update, and players want to find it. \\xa0Luckily, this Java seed gives gamers that opportunity. \\xa0It also has a very unique biome generation.Seed: 6705098208300174216agent_1: What are the two major biomes in Minecraft 1.19 and how do they differ from Deep Dark and Mangrove Swamp?\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df[\"agent_1\"][1400700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2e83014-4dd7-4388-a9f4-4e48c932c981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agent_2: In Minecraft 1.19, there are two new biomes added: \"Deep Dark\" and \"Mangrove Swamp\". The Deep Dark biome provides reasons to visit it but can be difficult to find due to its location near a swamp. Meanwhile, the Mangrove Swamp biome has only one problem - it can be difficult to find as well. Minecraft seeds like -457009213479927390, -6709148406763899126, and -2110863992403414331 can be used to trace a Deep Dark with just the Skulk and Warden. These seeds contain good loot but do not necessarily have Mangrove Swamp as they are located in swamps instead of in the swamp biome. The seeds -2110863992403414331 and 1450778142214593647 can be used to find Mangrove Swamp by using a village as an alternative spawn point. The seeds -6709148406763899126 and 6705'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df[\"agent_2\"][1400700]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362406fe-3022-4d03-987e-bf2e1a6ac1ac",
   "metadata": {},
   "source": [
    "### Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e488c1af-51a5-4674-a23d-b8dc88db0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = '../Saved_Models/Sky/fine-tuned-bert-sentiment_2024_10_05_0'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddfc833-c36c-4103-a16a-cc8a5660e49f",
   "metadata": {},
   "source": [
    "#### Normal Search Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9963382-6a6f-4718-977f-b1bd3a81dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Reinforcement learning with human feedback?\"\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d94dbc-f4d6-43ed-804e-60cf322d2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "def get_google_search_links(query):\n",
    "    return [link for link in search(query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf38a1b-1a84-49e7-a7c2-b93b6f7a0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_latex(text):\n",
    "    # Remove LaTeX commands like {\\\\displaystyle}, {\\\\text{}}, etc.\n",
    "    cleaned_text = re.sub(r'\\\\displaystyle|\\\\text\\{.*?\\}', '', text)\n",
    "    \n",
    "    # Remove any LaTeX curly braces and unnecessary whitespaces\n",
    "    cleaned_text = re.sub(r'\\\\[a-z]+|{|}', '', cleaned_text)\n",
    "    \n",
    "    # Replace LaTeX-specific representations like \\\\dots with their equivalent\n",
    "    cleaned_text = re.sub(r'\\\\dots', '...', cleaned_text)\n",
    "    \n",
    "    # Remove multiple spaces introduced by LaTeX removal\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2576fb34-3e2f-4892-b6f2-c5ea548e65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_links = get_google_search_links(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f319701-0ec5-466d-875d-d0c4a6f53d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://arxiv.org/abs/2201.08102',\n",
       " 'https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback',\n",
       " 'https://thisisrishi.medium.com/reinforcement-learning-with-human-feedback-in-llms-a-comprehensive-guide-771b381e94e7',\n",
       " 'https://encord.com/blog/guide-to-rlhf/',\n",
       " 'https://arxiv.org/abs/1709.03969',\n",
       " 'https://huggingface.co/learn/deep-rl-course/en/unitbonus3/rlhf',\n",
       " 'https://manikanthgoud123.medium.com/what-is-rlhf-reinforcement-learning-from-human-feedback-d0ec88e0866c',\n",
       " 'https://annotationbox.com/reinforcement-learning-from-human-feedback/',\n",
       " 'https://bdtechtalks.com/2023/01/16/what-is-rlhf/',\n",
       " 'https://www.leewayhertz.com/reinforcement-learning-from-human-feedback/',\n",
       " 'https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c86d4f3-edb3-47d4-b03d-b68dd637592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadWithReturnValue(Thread):\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs={}, Verbose=None):\n",
    "        Thread.__init__(self, group, target, name, args, kwargs)\n",
    "        self._return = None\n",
    "\n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args,\n",
    "                                                **self._kwargs)\n",
    "    def join(self, *args):\n",
    "        Thread.join(self, *args)\n",
    "        return self._return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "082e8261-7f77-43db-bb06-c37bdf97251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_mining(website):\n",
    "    filtered_content = \"\"\n",
    "    if (\".gov\" not in website) or (\"linkedin.com\" not in website) or (\"reddit.com\" not in website):\n",
    "        URL = website\n",
    "        r = requests.get(URL) \n",
    "        soup = BeautifulSoup(r.content, 'html5lib')\n",
    "        for tag in soup(['nav', 'header', 'footer', 'script', 'style', 'aside']):\n",
    "            tag.decompose()\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li','strong']):\n",
    "            filtered_content = filtered_content+tag.get_text()\n",
    "    return filtered_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21937f70-d5e8-4699-8aee-5a4acd65aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(cleaned_data):\n",
    "    ids_list = []\n",
    "    final_chunks = []\n",
    "    random_number = random.randint(0,10000000000)\n",
    "    loop = math.ceil(len(cleaned_data)/2000)\n",
    "    for i in range(0,loop):\n",
    "        if i ==0:\n",
    "            final_chunks.append(cleaned_data[(i*2000):(i+1)*2000])\n",
    "        else:\n",
    "            final_chunks.append(cleaned_data[(i*2000)-500:(i+1)*2000])\n",
    "        ids_list.append(str(random_number+(i/50)))\n",
    "    return (final_chunks,ids_list)\n",
    "    # return final_chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3913f4c0-37f2-4856-ae71-ec90ec86f56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|              chunks|          ids|\n",
      "+--------------------+-------------+\n",
      "|Computer Science ...|9962182769.02|\n",
      "|Supervised learni...| 678886517.02|\n",
      "|Sign upSign inSig...|7893596736.02|\n",
      "|Please enable Jav...|9379303736.02|\n",
      "|Computer Science ...|         NULL|\n",
      "|Deep RL Course do...|9585043200.02|\n",
      "|Sign upSign inSig...|6576366173.02|\n",
      "|You have reached ...|         NULL|\n",
      "|HomeBlogTips & Tr...|5751933141.02|\n",
      "|Reinforcement Lea...| 606611957.02|\n",
      "|What is Cloud Com...|2717412571.02|\n",
      "+--------------------+-------------+\n",
      "\n",
      "Execution time: 2.327502965927124 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "start_time = time.time()\n",
    "# spark = SparkSession.builder.appName(\"WebScrapingWithPySpark\").getOrCreate()\n",
    "threads_list = [ThreadWithReturnValue(target=data_mining, args=(website,)) for website in website_links]\n",
    "[thread.start() for thread in threads_list]\n",
    "fetched_data = [thread.join() for thread in threads_list]\n",
    "\n",
    "cleaned_data_udf = udf(clean_latex, StringType())\n",
    "# create_chunks_udf = udf(create_chunks, ArrayType(StringType()), ArrayType(StringType()))\n",
    "schema = StructType([\n",
    "    StructField(\"chunks\", ArrayType(StringType())),\n",
    "    StructField(\"ids\", ArrayType(StringType()))\n",
    "])\n",
    "create_chunks_udf = udf(create_chunks, schema)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .appName(\"WebScrapingWithPySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "temp_df = list(zip(website_links, fetched_data))\n",
    "uncleaned_df = spark.createDataFrame(temp_df, [\"url\", \"uncleaned_data\"])\n",
    "cleaned_df = uncleaned_df.withColumn(\"cleaned_data\", cleaned_data_udf(\"uncleaned_data\"))\n",
    "clean_df = cleaned_df.withColumn(\"processed\", create_chunks_udf(\"cleaned_data\"))\n",
    "\n",
    "clean_df = clean_df.select(\n",
    "    F.col('processed.chunks').getItem(0).alias(\"chunks\"),\n",
    "    F.col('processed.ids').getItem(1).alias(\"ids\")\n",
    ")\n",
    "\n",
    "clean_df.show()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83e1e3d5-0772-4acf-ad33-f9d398339e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = clean_df.collect()\n",
    "\n",
    "# Access the nth row (for example, 3rd row, index 2)\n",
    "# nth_row = rows[4]\n",
    "# nth_row['processed'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a939824-3a32-4f2c-a00d-50359f688f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(chunks='Computer Science > Machine LearningarXiv:2201.08102Title:Safe Deep RL in 3D Environments using Human FeedbackSubmission history[v1][v2]Access Paper:View PDFTeX SourceOther FormatsAncillary files (details):Cliff edge environment/images/image1.mp4Cliff edge environment/images/image10.mp4Cliff edge environment/images/image11.mp4Cliff edge environment/images/image12.mp4Cliff edge environment/images/image2.mp4Cliff edge environment/images/image3.mp4Cliff edge environment/images/image4.pngCliff edge environment/images/image5.mp4Cliff edge environment/images/image6.mp4Cliff edge environment/images/image7.pngCliff edge environment/images/image8.pngCliff edge environment/images/image9.mp4Cliff edge environment/instructions.htmlDangerous blocks environment/images/image1.mp4Dangerous blocks environment/images/image10.mp4Dangerous blocks environment/images/image11.pngDangerous blocks environment/images/image12.mp4Dangerous blocks environment/images/image2.pngDangerous blocks environment/images/image3.mp4Dangerous blocks environment/images/image4.pngDangerous blocks environment/images/image5.mp4Dangerous blocks environment/images/image6.mp4Dangerous blocks environment/images/image7.mp4Dangerous blocks environment/images/image8.mp4Dangerous blocks environment/images/image9.mp4Dangerous blocks environment/instructions.html(21 additional files not shown) You must enabled JavaScript to view entire file list.References & CitationsNASA ADSGoogle ScholarSemantic ScholarDBLP - CS BibliographyBibTeX formatted citationBookmarkBibliographic and Citation ToolsCode, Data and Media Associated with this ArticleDemosRecommenders and Search ToolsAuthorVenueInstitutionTopicarXivLabs: experimental projects with community collaboratorsarXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data priva', ids='2251963347.02'),\n",
       " Row(chunks='Supervised learningUnsupervised learningSemi-supervised learningSelf-supervised learningReinforcement learningMeta-learningOnline learningBatch learningCurriculum learningRule-based learningNeuro-symbolic AINeuromorphic engineeringQuantum machine learningClassificationGenerative modelingRegressionClusteringDimensionality reductionDensity estimationAnomaly detectionData cleaningAutoMLAssociation rulesSemantic analysisStructured predictionFeature engineeringFeature learningLearning to rankGrammar inductionOntology learningMultimodal learningApprenticeship learningDecision treesEnsembles Bagging Boosting Random forestBaggingBoostingRandom forestk-NNLinear regressionNaive BayesArtificial neural networksLogistic regressionPerceptronRelevance vector machine (RVM)Support vector machine (SVM)BIRCHCUREHierarchicalk-meansFuzzyExpectation–maximization (EM)DBSCANOPTICSMean shiftFactor analysisCCAICALDANMFPCAPGDt-SNESDLGraphical models Bayes net Conditional random field Hidden MarkovBayes netConditional random fieldHidden MarkovRANSACk-NNLocal outlier factorIsolation forestAutoencoderDeep learningFeedforward neural networkRecurrent neural network LSTM GRU ESN reservoir computingLSTMGRUESNreservoir computingBoltzmann machine RestrictedRestrictedGANDiffusion modelSOMConvolutional neural network U-Net LeNet AlexNet DeepDreamU-NetLeNetAlexNetDeepDreamNeural radiance fieldTransformer VisionVisionMambaSpiking neural networkMemtransistorElectrochemical RAM (ECRAM)Q-learningSARSATemporal difference (TD)Multi-agent Self-playSelf-playActive learningCrowdsourcingHuman-in-the-loopRLHFCoefficient of determinationConfusion matrixLearning curveROC curveKernel machinesBias–variance tradeoffComputational learning theoryEmpirical risk minimizationOccam learningPAC learningStatistical learningVC theoryECML PKDDNeurIPSICMLICLRIJCAIMLJMLRGlossary of artificial intelligenceList of datasets for machine-learning research List of datasets in computer vision and image processingList of datasets in comput', ids='2067070779.02'),\n",
       " Row(chunks='Sign upSign inSign upSign inReinforcement Learning with Human Feedback in LLMs: A Comprehensive GuideRishiFollow--1ListenShareIntroductionLarge Language Models (LLMs) are everywhere these days, helping us process and understand natural language. They have the potential to change the game in various industries and make human-computer interactions even better. However, we need to make sure they align with human values and generate reliable and diverse outputs. To achieve this, we use Reinforcement Learning with Human Feedback (RLHF) to train LLMs and incorporate human preferences. This technique is crucial to improving their safety and helpfulness.In this guide, we’ll take a deep dive into an important process called RLHF and its role in developing modern LLMs. We’ll walk through the steps involved in RLHF, compare how it’s used in popular models like ChatGPT and Llama 2, and talk about the alternatives and limitations of RLHF. By the end of this guide, you’ll have a clear understanding of how RLHF helps create language models that are better aligned with human intentions.Let’s get started!Table of ContentsWhat is Reinforcement Learning with Human Feedback (RLHF)?The Importance of RLHF in LLM TrainingThe LLM Training PipelineLimitations and Challenges of RLHFFuture Directions and Ongoing ResearchConclusionWhat is Reinforcement Learning with Human Feedback (RLHF)?Reinforcement Learning with Human Feedback (RLHF) is a technique used to train LLMs by taking into account feedback from real people like you and me. The idea is to make LLMs more in line with human values and generate better outputs that people will like and trust.Use CaseUse CaseLlama 2, developed by Meta AI, goes through a similar supervised finetuning step as ChatGPT, but it has some differences in its RLHF process. Instead of just one reward model, Llama 2 has two that focus on helpfulness and safety. These scores are combined to create a final reward function for optimizing the model. Llama 2 also uses a', ids='3870490494.02'),\n",
       " Row(chunks='Please enable JavaScript to view this site.ContentsMachine Learning With Reinforcement LearningMachine Learning With Reinforcement LearningSurpassing HumansSurpassing HumansHuman-In-The-Loop: RLHFHuman-In-The-Loop: RLHFTraining Computer Vision With Reinforcement LearningTraining Computer Vision With Reinforcement LearningFine-Tuning Existing Models With Reinforcement LearningFine-Tuning Existing Models With Reinforcement LearningRLHF: Key TakeawaysRLHF: Key TakeawaysRLHF: Frequently Asked Questions RLHF: Frequently Asked Questions Guide to Reinforcement Learning from Human Feedback (RLHF) for Computer VisionPower your AI models with the right dataright dataAutomate your data curation, annotation and label validation workflows.ContentsMachine Learning With Reinforcement LearningMachine Learning With Reinforcement LearningSurpassing HumansSurpassing HumansHuman-In-The-Loop: RLHFHuman-In-The-Loop: RLHFTraining Computer Vision With Reinforcement LearningTraining Computer Vision With Reinforcement LearningFine-Tuning Existing Models With Reinforcement LearningFine-Tuning Existing Models With Reinforcement LearningRLHF: Key TakeawaysRLHF: Key TakeawaysRLHF: Frequently Asked Questions RLHF: Frequently Asked Questions Nikolaj BuhlReinforcement Learning (RL) is a machine learning training method that relies on the state of the environment to train intelligent agents. The RL training paradigm induces a form of self-learning where the agent can understand changing environments and modify its output accordingly.Reinforcement learning from human feedback (RLHF) is an extended form of the conventional RL technique. It adds a human feedback component to the overall architecture. This human-guided training helps fine-tune the model and build applications like InstructGPT. These models converge faster and eventually learn better patterns.RLHF has several interesting applications across various fields of artificial intelligence, especially Computer Vision (CV). It trains CV models by', ids='4420089998.02'),\n",
       " Row(chunks=\"Computer Science > Artificial IntelligencearXiv:1709.03969Title:Explore, Exploit or Listen: Combining Human Feedback and Policy Model to Speed up Deep Reinforcement Learning in 3D WorldsSubmission history[v1][v2]Access Paper:View PDFTeX SourceOther FormatsReferences & CitationsNASA ADSGoogle ScholarSemantic ScholarDBLP - CS BibliographyBibTeX formatted citationBookmarkBibliographic and Citation ToolsCode, Data and Media Associated with this ArticleDemosRecommenders and Search ToolsAuthorVenueInstitutionTopicarXivLabs: experimental projects with community collaboratorsarXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.Learn more about arXivLabs\", ids=None),\n",
       " Row(chunks='Deep RL Course documentation RLHFDeep RL Courseand get access to the augmented documentation experience to get started RLHFReinforcement learning from human feedback (RLHF) is a methodology for integrating human data labels into a RL-based optimization process. It is motivated by the challenge of modeling human preferences.methodology for integrating human data labels into a RL-based optimization processchallenge of modeling human preferencesFor many questions, even if you could try and write down an equation for one ideal, humans differ on their preferences.Updating models based on measured data is an avenue to try and alleviate these inherently human ML problems.based on measured data is an avenue to try and alleviate these inherently human ML problems Start Learning about RLHFTo start learning about RLHF:Read this introduction: Illustrating Reinforcement Learning from Human Feedback (RLHF).Read this introduction: Illustrating Reinforcement Learning from Human Feedback (RLHF).Watch the recorded live we did some weeks ago, where Nathan covered the basics of Reinforcement Learning from Human Feedback (RLHF) and how this technology is being used to enable state-of-the-art ML tools like ChatGPT. Most of the talk is an overview of the interconnected ML models. It covers the basics of Natural Language Processing and RL and how RLHF is used on large language models. We then conclude with open questions in RLHF.Watch the recorded live we did some weeks ago, where Nathan covered the basics of Reinforcement Learning from Human Feedback (RLHF) and how this technology is being used to enable state-of-the-art ML tools like ChatGPT. Most of the talk is an overview of the interconnected ML models. It covers the basics of Natural Language Processing and RL and how RLHF is used on large language models. We then conclude with open questions in RLHF.Read other blogs on this topic, such as Closed-API vs Open-source continues: RLHF, ChatGPT, data moats. Let us know if there are more y', ids='7254214660.02'),\n",
       " Row(chunks='Sign upSign inSign upSign inWhat is RLHF — Reinforcement Learning from Human FeedbackManikanthFollow--ListenShareReinforcement Learning from Human Feedback (RLHF) involves training AI through human input, enhancing the AI’s comprehension of human values and preferences, in contrast to conventional reinforcement learning that depends on predefined goals.Context:ContextIntroduction to RLHFHow RLHF WorksBenefits of RLHFChallenges and ConsiderationsRLHF in PracticeRAG vs RLHF.Future of RLHFIntroduction to RLHFRLHF operates on a fundamental yet potent principle: harnessing human feedback to steer the learning trajectory of AI models. This methodology generally encompasses several essential steps:Initial Point: The model learns the basics by looking at lots of examples in its special training set. This helps it understand how to do its job.Initial Point:Human Involvement: Once the model has learned some basics, people start talking to it. They tell the model how well it’s doing by ranking, fixing mistakes, or making simple choices between two options like and dislike.Human Involvement:Using Feedback: The model takes the feedback from people and uses it to make itself better. This could mean directly changing how it learns or creating a system that understands the feedback and improves its future predictions.Using Feedback:Getting Better Step by Step: The model keeps getting feedback and making changes to improve itself. Each time it does this, it gets a bit closer to doing things the way people want it to.Getting Better Step by Step:How RLHF WorksPretraining Language ModelsPretraining Language ModelsStarting Out: The first step is to teach a language model by showing it tons of text. Big organizations like OpenAI, Anthropic, and DeepMind use models with a lot of learning power, with parameters ranging from 10 million to a whopping 280 billion. This basic model is like the foundation, helping the model understand and create text that’s similar to how humans talk.Gathering ', ids='2984570990.02'),\n",
       " Row(chunks=\"You have reached an error page on WPX.net If you are seeing this page, it is because there is no website installed on this domain yet. Find the right hosting plan for you on the world's fastest WordPress hosting service here.MOST TRUSTEDTrusted by Trustpilot.comBEST SUPPORTTested by Mathew WoodwardBEST SEOTested by Matt DiggityFASTEST WPTested by Kevin Ohashi\", ids=None),\n",
       " Row(chunks='HomeBlogTips & TricksWhat isInterviewsReviewsAbout About TechTalks About Ben Dickson Write for TechTalks About TechTalksAbout Ben DicksonWrite for TechTalksHomeBlog The (not so) hidden costs of AI’s “Bigger is Better” paradigm OpenAI could undercut Microsoft with new ChatGPT app for Windows Nvidia is playing a smart game with its Nemotron-70B model Mistral expands its reach in the SLM space with Ministral models The price of OpenAI’s $150 billion valuation The (not so) hidden costs of AI’s “Bigger is Better” paradigm OpenAI could undercut Microsoft with new ChatGPT app for Windows Nvidia is playing a smart game with its Nemotron-70B model Mistral expands its reach in the SLM space with Ministral models The price of OpenAI’s $150 billion valuation The (not so) hidden costs of AI’s “Bigger is Better” paradigmOpenAI could undercut Microsoft with new ChatGPT app for WindowsNvidia is playing a smart game with its Nemotron-70B modelMistral expands its reach in the SLM space with Ministral modelsThe price of OpenAI’s $150 billion valuationTips & Tricks 4 ways to improve the retrieval of your RAG pipeline How to analyze and fix errors in LLM applications How to approach LLMs and generative AI tools in everyday tasks How to create fine-tuned LLMs with ChatGPT Fine-tune a Llama-2 language model with a single instruction 4 ways to improve the retrieval of your RAG pipeline How to analyze and fix errors in LLM applications How to approach LLMs and generative AI tools in everyday tasks How to create fine-tuned LLMs with ChatGPT Fine-tune a Llama-2 language model with a single instruction 4 ways to improve the retrieval of your RAG pipelineHow to analyze and fix errors in LLM applicationsHow to approach LLMs and generative AI tools in everyday tasksHow to create fine-tuned LLMs with ChatGPTFine-tune a Llama-2 language model with a single instructionWhat is What to know about open-source alternatives to GPT-4 Vision The complete guide to LLM compression A simple guide to gradient ', ids='4408816325.02'),\n",
       " Row(chunks='Reinforcement Learning from Human Feedback (RLHF): Working, applications, benefits and RLAIF Twitter Facebook Linkedin While these technologies hold the potential for transformative outcomes, there are also associated risks. The development of Reinforcement Learning from Human Feedback (RLHF) represents a significant breakthrough in ensuring that AI models align with human values, delivering helpful, honest and harmless responses. Given the concerns about the speed and scope of the deployment of generative AI, it is now more important than ever to incorporate an ongoing, efficient human feedback loop.Reinforcement learning from human feedback is a machine-learning approach that leverages a combination of human feedback and reinforcement learning to train AI models. Reinforcement learning involves training an AI model to learn through trial and error, where the model is rewarded for making correct decisions and penalized for making incorrect ones.However, reinforcement learning has its own limitations. For instance, defining a reward function that captures all aspects of human preferences and values may be challenging, making it difficult to ensure that the model aligns with human values. RLHF addresses this challenge by integrating human feedback into the training process, making aligning the model with human values more effective. By providing feedback on the model’s output, humans can help the model learn faster and more accurately, reducing the risk of harmful errors. For instance, when humans provide feedback on the model’s output, they can identify cases where the model provides inappropriate, biased, or toxic responses and provide corrective feedback to help the model learn.Furthermore, RLHF can help overcome the issue of sample inefficiency in reinforcement learning. Sample inefficiency is a problem where reinforcement learning requires many iterations to learn a task, making it time-consuming and expensive. However, with the integration of human feedback, th', ids='1331099454.02'),\n",
       " Row(chunks='What is Cloud Computing?Cloud Computing Concepts HubGenerative AIMachine LearningWhat is RLHF?What is RLHF?Reinforcement learning from human feedback (RLHF) is a machine learning (ML) technique that uses human feedback to optimize ML models to self-learn more efficiently. Reinforcement learning (RL) techniques train software to make decisions that maximize rewards, making their outcomes more accurate. RLHF incorporates human feedback in the rewards function, so the ML model can perform tasks more aligned with human goals, wants, and needs. RLHF is used throughout generative artificial intelligence (generative AI) applications, including in large language models (LLM).Read about machine learningRead about reinforcement learningRead about generative AIRead about large language modelsWhy is RLHF important?The applications of artificial intelligence (AI) are broad-ranging, from self-driving cars to natural language processing (NLP), stock market predictors, and retail personalization services. No matter the given application, the goal of AI is ultimately to mimic human responses, behaviors, and decision-making. The ML model must encode human input as training data so that the AI mimics humans more closely when completing complex tasks.RLHF is a specific technique that is used in training AI systems to appear more human, alongside other techniques such as supervised and unsupervised learning. First, the model’s responses are compared to the responses of a human. Then a human assesses the quality of different responses from the machine, scoring which responses sound more human. The score can be based on innately human qualities, such as friendliness, the right degree of contextualization, and mood. RLHF is prominent in natural language understanding, but it’s also used across other generative AI applications.Read about artificial intelligenceRead about natural language processingRead about the difference between supervised and unsupervised learningEnhances AI performanceE', ids='8870797103.02')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07089ae8-f894-4cdb-a3b2-51275b6702cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(chunks='Sign upSign inSign upSign inReinforcement Learning with Human Feedback in LLMs: A Comprehensive GuideRishiFollow--1ListenShareIntroductionLarge Language Models (LLMs) are everywhere these days, helping us process and understand natural language. They have the potential to change the game in various industries and make human-computer interactions even better. However, we need to make sure they align with human values and generate reliable and diverse outputs. To achieve this, we use Reinforcement Learning with Human Feedback (RLHF) to train LLMs and incorporate human preferences. This technique is crucial to improving their safety and helpfulness.In this guide, we’ll take a deep dive into an important process called RLHF and its role in developing modern LLMs. We’ll walk through the steps involved in RLHF, compare how it’s used in popular models like ChatGPT and Llama 2, and talk about the alternatives and limitations of RLHF. By the end of this guide, you’ll have a clear understanding of how RLHF helps create language models that are better aligned with human intentions.Let’s get started!Table of ContentsWhat is Reinforcement Learning with Human Feedback (RLHF)?The Importance of RLHF in LLM TrainingThe LLM Training PipelineLimitations and Challenges of RLHFFuture Directions and Ongoing ResearchConclusionWhat is Reinforcement Learning with Human Feedback (RLHF)?Reinforcement Learning with Human Feedback (RLHF) is a technique used to train LLMs by taking into account feedback from real people like you and me. The idea is to make LLMs more in line with human values and generate better outputs that people will like and trust.Use CaseUse CaseLlama 2, developed by Meta AI, goes through a similar supervised finetuning step as ChatGPT, but it has some differences in its RLHF process. Instead of just one reward model, Llama 2 has two that focus on helpfulness and safety. These scores are combined to create a final reward function for optimizing the model. Llama 2 also uses a', ids='3870490494.02')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 57883)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"C:\\Users\\Akshay\\anaconda3\\envs\\tf\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rows[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "384c453f-847a-4186-a7ce-5a4cae13b63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "19 19\n",
      "4 4\n",
      "8 8\n",
      "1 1\n",
      "3 3\n",
      "5 5\n",
      "1 1\n",
      "8 8\n",
      "29 29\n",
      "5 5\n"
     ]
    }
   ],
   "source": [
    "for r in rows:\n",
    "    print(len(r[3][0]),len(r[3][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3518715-8cf5-4707-90fe-0b379148f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session1.92173671\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6113c-2eca-4821-934b-bf76847e7001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47bee1-c661-420b-909e-026931c08568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc18e9-5fc9-4c09-b172-0ad28fb7c8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09bd28d-c6d1-47b1-ad6e-cf2380d32640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d3888b2-bc0f-4ee4-be2e-981fd327c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16da69cd-0bf2-4e73-ba07-10569a95ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma_client.delete_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "354cee1d-9c35-4574-b4cc-e5efcd66dcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7d367ba-75fa-4f23-8332-02a75407081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.upsert(\n",
    "    documents=final_chunks,\n",
    "    ids=ids_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db8fe6dd-67a3-4d57-9452-980ed3659b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.07861828804016113 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query], # Chroma will embed this for you\n",
    "    n_results=2 # how many results to return\n",
    ")\n",
    "# print(results)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total time taken\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "741fb75b-c061-4ee6-bb47-821d0284486f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7082719453848038, 0.7667292356491089]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['distances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed059ac8-a779-4f76-ac45-e3d950b84a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agent_2:Yes, he has a Canadian passport.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"<context>{}agent_1:{}\".format(results[\"documents\"][0][0],query), return_tensors='pt',max_length=1024, truncation=True).to(device)\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=512, \n",
    "    num_beams=10, \n",
    "    early_stopping=True, \n",
    "    no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
    "    num_return_sequences=1,  # Number of sequences to return\n",
    "    temperature=0.7,  # Sampling temperature\n",
    "    top_k=50,  # Top-K sampling\n",
    "    top_p=0.9  # Top-p (nucleus) sampling\n",
    ")\n",
    "resp = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "364eb77e-d642-494e-8572-bc8de7314424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output: agentagent!-->omes paradise, swanky Mauritius bungalow, and a stunning villa in Goa. how Akshay transformed from a humble beginning to owning some of the world’s most luxurious homes. With exclusive insights into his glamorous residences and the stunning interiors crafted by his wife Twinkle Khanna, get ready to be amazed by the opulence and grandeur that define the Khiladi’s lifestyle! of Akshay Kumar & Home Address of Akshay Kumar was Home Address Akshay Kumar was born on the 9th of September 1967. He was born in Amritsar, Punjab, India and moved to Delhi where he spent most of his childhood. When his father retired from the army to join UNICEF he moved to Mumbai. Rajiv Hari Om Bhatia made his way into Hindi cinema in the early 1990s, after a series of flops came the breakthrough role in Khiladi directed by Abas-Mastan established Akshay Kumar as an action hero. Akshay Kumar adopted the professional name of Akshay Kumar before stepping into movies. His charming smile and lean physique made him a ladies' man. Even today at 52, he is of the fittest and sexiest actors in Bollywood. He Service Guarantee Or Painting Free Get a rental agreement with doorstep delivery Find the BEST deals and get unbelievable discountsCOUNTS directly from builders! 5-Star rated painters, premium paints and services at the BEST PRICES! a commercial superstar, Akshay Kumar has made many content-rich movies in recent times, Rustom, Padman, and Kesari are a few of the fantastic movies that propelled him into newfound popularity. He is known for his discipline and punctuality, as well as for being a prankster on sets. He does not party hard nor do in indulge in alcohol or drugs, so what does he spend his money on? Akshay Kumar House Locations Akshay Kumar Locations Akshfront Juhu bungalow, A home in Mauritius, A in Maurit, Four more flats in Andheri some of the smart real-estate investments he has made over the years. Akshay has invested in properties globally and owns some of the most expensive homes in the world. Chef tostar Chef Superstariv Bhatia as a in a small in Bangkok before the shiny of B\n",
      "Confidence Score: 0.9339369535446167\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(\"<context>{}agent_1:{}\".format(results[\"documents\"][0][0],query), return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "\n",
    "# Forward pass through the model to get the logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get the predicted token ids from the logits (the most likely tokens)\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Calculate the probabilities (softmax) for the predicted tokens\n",
    "probabilities = softmax(logits, dim=-1)\n",
    "\n",
    "# Get the probabilities of the predicted tokens\n",
    "predicted_probabilities = probabilities.gather(2, predicted_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# Calculate the confidence score as the average probability of the predicted tokens\n",
    "confidence_score = predicted_probabilities.mean().item()\n",
    "\n",
    "print(\"Generated Output:\", tokenizer.decode(predicted_ids[0], skip_special_tokens=True).replace(\"\\n\",\"\"))\n",
    "print(\"Confidence Score:\", confidence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0105b-adfe-44ec-9801-724f68dfad6c",
   "metadata": {},
   "source": [
    "# Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0717a22-14d5-4e12-a41d-b9f13ea725d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('chroma.sqlite3') \n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30f76bec-da85-450f-83da-85887aeb5a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('migrations',), ('embeddings_queue',), ('embeddings_queue_config',), ('collection_metadata',), ('segment_metadata',), ('tenants',), ('databases',), ('collections',), ('maintenance_log',), ('segments',), ('embeddings',), ('embedding_metadata',), ('max_seq_id',), ('embedding_fulltext_search',), ('embedding_fulltext_search_data',), ('embedding_fulltext_search_idx',), ('embedding_fulltext_search_content',), ('embedding_fulltext_search_docsize',), ('embedding_fulltext_search_config',)]\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"SELECT name FROM sqlite_master  \n",
    "  WHERE type='table';\"\"\"\n",
    "cursor.execute(sql_query)\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e68410c5-f82f-435b-a6eb-e3e2dfa1d0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x296dd42a8f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = \"\"\"SELECT * FROM collections;\"\"\"\n",
    "cursor.execute(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3adf4fb-96b8-402b-bb97-216572a4eda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1c13d8d4-830b-4def-928f-fb4fc943daa2', 'my_collection', 384, '00000000-0000-0000-0000-000000000000', '{\"hnsw_configuration\": {\"space\": \"l2\", \"ef_construction\": 100, \"ef_search\": 10, \"num_threads\": 28, \"M\": 16, \"resize_factor\": 1.2, \"batch_size\": 100, \"sync_threshold\": 1000, \"_type\": \"HNSWConfigurationInternal\"}, \"_type\": \"CollectionConfigurationInternal\"}')]\n"
     ]
    }
   ],
   "source": [
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b89d3-b08c-4269-940e-19cf0d9c189e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
